{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AILS Statistical Analysis\n",
    "\n",
    "This notebook performs comprehensive statistical analysis for the AILS paper:\n",
    "- Paired t-tests for performance comparison\n",
    "- Cohen's d effect size calculations\n",
    "- ANOVA for multi-group comparisons\n",
    "- Confidence interval estimation\n",
    "- Tukey HSD post-hoc analysis\n",
    "\n",
    "**Author:** Amr Elshahed  \n",
    "**Institution:** Universiti Sains Malaysia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Set style\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"AILS Statistical Analysis Notebook\")\n",
    "print(f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Experiment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the most recent results file\n",
    "results_dir = '../data/results/'\n",
    "\n",
    "# Try to find existing results\n",
    "synthetic_files = sorted(glob.glob(f'{results_dir}synthetic_results_*.csv'))\n",
    "movingai_files = sorted(glob.glob(f'{results_dir}movingai_results_*.csv'))\n",
    "\n",
    "df = None\n",
    "\n",
    "if synthetic_files:\n",
    "    df = pd.read_csv(synthetic_files[-1])\n",
    "    print(f\"Loaded: {synthetic_files[-1]}\")\n",
    "elif movingai_files:\n",
    "    df = pd.read_csv(movingai_files[-1])\n",
    "    print(f\"Loaded: {movingai_files[-1]}\")\n",
    "else:\n",
    "    # Generate sample data for demonstration\n",
    "    print(\"No existing results found. Generating sample data...\")\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    n_samples = 500\n",
    "    methods = ['standard_astar', 'ails_base', 'ails_adaptive']\n",
    "    \n",
    "    data = []\n",
    "    for method in methods:\n",
    "        for i in range(n_samples):\n",
    "            base_time = 10 if method == 'standard_astar' else (7 if method == 'ails_base' else 5)\n",
    "            base_nodes = 1000 if method == 'standard_astar' else (700 if method == 'ails_base' else 500)\n",
    "            \n",
    "            data.append({\n",
    "                'method': method,\n",
    "                'pair_id': i,\n",
    "                'time_ms': base_time + np.random.exponential(base_time * 0.3),\n",
    "                'nodes_visited': int(base_nodes + np.random.exponential(base_nodes * 0.2)),\n",
    "                'path_found': True,\n",
    "                'grid_size': np.random.choice([100, 200, 300]),\n",
    "                'pattern': np.random.choice(['uniform', 'clustered'])\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    print(f\"Generated {len(df)} sample records\")\n",
    "\n",
    "# Filter to successful paths only\n",
    "df = df[df['path_found']].copy()\n",
    "print(f\"\\nTotal records: {len(df)}\")\n",
    "print(f\"Methods: {df['method'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics by method\n",
    "summary = df.groupby('method').agg({\n",
    "    'time_ms': ['mean', 'std', 'median', 'min', 'max', 'count'],\n",
    "    'nodes_visited': ['mean', 'std', 'median', 'min', 'max']\n",
    "}).round(3)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DESCRIPTIVE STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "print(summary.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Paired t-Tests\n",
    "\n",
    "Compare AILS variants against standard A* using paired t-tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paired_ttest(df, method1, method2, metric):\n",
    "    \"\"\"Perform paired t-test between two methods.\"\"\"\n",
    "    df1 = df[df['method'] == method1].sort_values('pair_id')\n",
    "    df2 = df[df['method'] == method2].sort_values('pair_id')\n",
    "    \n",
    "    # Match by pair_id\n",
    "    common_pairs = set(df1['pair_id']) & set(df2['pair_id'])\n",
    "    \n",
    "    if len(common_pairs) < 10:\n",
    "        return None\n",
    "    \n",
    "    v1 = df1[df1['pair_id'].isin(common_pairs)][metric].values\n",
    "    v2 = df2[df2['pair_id'].isin(common_pairs)][metric].values\n",
    "    \n",
    "    t_stat, p_value = stats.ttest_rel(v1, v2)\n",
    "    \n",
    "    # Cohen's d for paired samples\n",
    "    diff = v1 - v2\n",
    "    cohens_d = np.mean(diff) / np.std(diff, ddof=1)\n",
    "    \n",
    "    return {\n",
    "        'n_pairs': len(common_pairs),\n",
    "        'mean_1': np.mean(v1),\n",
    "        'mean_2': np.mean(v2),\n",
    "        'mean_diff': np.mean(diff),\n",
    "        't_statistic': t_stat,\n",
    "        'p_value': p_value,\n",
    "        'cohens_d': cohens_d,\n",
    "        'significant': p_value < 0.05\n",
    "    }\n",
    "\n",
    "# Run paired t-tests\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PAIRED T-TESTS (vs Standard A*)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "comparisons = [\n",
    "    ('standard_astar', 'ails_base'),\n",
    "    ('standard_astar', 'ails_adaptive'),\n",
    "    ('ails_base', 'ails_adaptive')\n",
    "]\n",
    "\n",
    "ttest_results = []\n",
    "\n",
    "for method1, method2 in comparisons:\n",
    "    for metric in ['time_ms', 'nodes_visited']:\n",
    "        result = paired_ttest(df, method1, method2, metric)\n",
    "        if result:\n",
    "            result['comparison'] = f\"{method1} vs {method2}\"\n",
    "            result['metric'] = metric\n",
    "            ttest_results.append(result)\n",
    "            \n",
    "            print(f\"\\n{result['comparison']} ({metric}):\")\n",
    "            print(f\"  n = {result['n_pairs']} pairs\")\n",
    "            print(f\"  Mean difference: {result['mean_diff']:.4f}\")\n",
    "            print(f\"  t-statistic: {result['t_statistic']:.4f}\")\n",
    "            print(f\"  p-value: {result['p_value']:.6f}\")\n",
    "            print(f\"  Cohen's d: {result['cohens_d']:.4f}\")\n",
    "            print(f\"  Significant (p<0.05): {result['significant']}\")\n",
    "\n",
    "df_ttest = pd.DataFrame(ttest_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Effect Size Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_cohens_d(d):\n",
    "    \"\"\"Interpret Cohen's d effect size.\"\"\"\n",
    "    d = abs(d)\n",
    "    if d < 0.2:\n",
    "        return \"Negligible\"\n",
    "    elif d < 0.5:\n",
    "        return \"Small\"\n",
    "    elif d < 0.8:\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"Large\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EFFECT SIZE INTERPRETATION\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "Cohen's d thresholds:\n",
    "  |d| < 0.2:  Negligible effect\n",
    "  0.2 <= |d| < 0.5:  Small effect\n",
    "  0.5 <= |d| < 0.8:  Medium effect\n",
    "  |d| >= 0.8:  Large effect\n",
    "\"\"\")\n",
    "\n",
    "if len(df_ttest) > 0:\n",
    "    print(\"\\nEffect sizes from our experiments:\")\n",
    "    for _, row in df_ttest.iterrows():\n",
    "        interp = interpret_cohens_d(row['cohens_d'])\n",
    "        print(f\"  {row['comparison']} ({row['metric']}): d={row['cohens_d']:.3f} ({interp})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ANOVA Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ONE-WAY ANOVA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for metric in ['time_ms', 'nodes_visited']:\n",
    "    groups = [df[df['method'] == m][metric].values for m in df['method'].unique()]\n",
    "    \n",
    "    f_stat, p_value = stats.f_oneway(*groups)\n",
    "    \n",
    "    print(f\"\\n{metric}:\")\n",
    "    print(f\"  F-statistic: {f_stat:.4f}\")\n",
    "    print(f\"  p-value: {p_value:.6f}\")\n",
    "    print(f\"  Significant difference between groups: {p_value < 0.05}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Tukey HSD Post-hoc Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TUKEY HSD POST-HOC ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for metric in ['time_ms', 'nodes_visited']:\n",
    "    print(f\"\\n{metric}:\")\n",
    "    \n",
    "    tukey = pairwise_tukeyhsd(\n",
    "        endog=df[metric],\n",
    "        groups=df['method'],\n",
    "        alpha=0.05\n",
    "    )\n",
    "    \n",
    "    print(tukey)\n",
    "    \n",
    "    # Save as DataFrame\n",
    "    tukey_df = pd.DataFrame(\n",
    "        data=tukey._results_table.data[1:],\n",
    "        columns=tukey._results_table.data[0]\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTukey HSD Summary for {metric}:\")\n",
    "    print(tukey_df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Confidence Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence_interval(data, confidence=0.95):\n",
    "    \"\"\"Calculate confidence interval for mean.\"\"\"\n",
    "    n = len(data)\n",
    "    mean = np.mean(data)\n",
    "    se = stats.sem(data)\n",
    "    h = se * stats.t.ppf((1 + confidence) / 2, n - 1)\n",
    "    return mean, mean - h, mean + h\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"95% CONFIDENCE INTERVALS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "ci_results = []\n",
    "\n",
    "for method in df['method'].unique():\n",
    "    df_m = df[df['method'] == method]\n",
    "    \n",
    "    for metric in ['time_ms', 'nodes_visited']:\n",
    "        mean, ci_low, ci_high = confidence_interval(df_m[metric])\n",
    "        ci_results.append({\n",
    "            'method': method,\n",
    "            'metric': metric,\n",
    "            'mean': mean,\n",
    "            'ci_lower': ci_low,\n",
    "            'ci_upper': ci_high,\n",
    "            'ci_width': ci_high - ci_low\n",
    "        })\n",
    "\n",
    "df_ci = pd.DataFrame(ci_results)\n",
    "print(df_ci.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# 1. Box plot - Time\n",
    "ax = axes[0, 0]\n",
    "df.boxplot(column='time_ms', by='method', ax=ax)\n",
    "ax.set_title('Execution Time by Method')\n",
    "ax.set_xlabel('Method')\n",
    "ax.set_ylabel('Time (ms)')\n",
    "plt.suptitle('')\n",
    "\n",
    "# 2. Box plot - Nodes\n",
    "ax = axes[0, 1]\n",
    "df.boxplot(column='nodes_visited', by='method', ax=ax)\n",
    "ax.set_title('Nodes Visited by Method')\n",
    "ax.set_xlabel('Method')\n",
    "ax.set_ylabel('Nodes')\n",
    "plt.suptitle('')\n",
    "\n",
    "# 3. Confidence interval plot - Time\n",
    "ax = axes[1, 0]\n",
    "df_time_ci = df_ci[df_ci['metric'] == 'time_ms']\n",
    "x = range(len(df_time_ci))\n",
    "ax.errorbar(x, df_time_ci['mean'], \n",
    "            yerr=[df_time_ci['mean'] - df_time_ci['ci_lower'],\n",
    "                  df_time_ci['ci_upper'] - df_time_ci['mean']],\n",
    "            fmt='o', capsize=5, capthick=2, markersize=8)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(df_time_ci['method'], rotation=45, ha='right')\n",
    "ax.set_ylabel('Time (ms)')\n",
    "ax.set_title('Mean Time with 95% CI')\n",
    "\n",
    "# 4. Confidence interval plot - Nodes\n",
    "ax = axes[1, 1]\n",
    "df_nodes_ci = df_ci[df_ci['metric'] == 'nodes_visited']\n",
    "ax.errorbar(x, df_nodes_ci['mean'], \n",
    "            yerr=[df_nodes_ci['mean'] - df_nodes_ci['ci_lower'],\n",
    "                  df_nodes_ci['ci_upper'] - df_nodes_ci['mean']],\n",
    "            fmt='o', capsize=5, capthick=2, markersize=8)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(df_nodes_ci['method'], rotation=45, ha='right')\n",
    "ax.set_ylabel('Nodes Visited')\n",
    "ax.set_title('Mean Nodes with 95% CI')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/results/statistical_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Generate LaTeX Tables for Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LATEX TABLES FOR PAPER\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Statistical summary table\n",
    "latex_stats = \"\"\"\\\\begin{table}[ht]\n",
    "\\\\centering\n",
    "\\\\caption{Statistical Comparison of Pathfinding Methods}\n",
    "\\\\label{tab:statistics}\n",
    "\\\\begin{tabular}{lcccccc}\n",
    "\\\\toprule\n",
    "Comparison & Metric & Mean Diff & t-stat & p-value & Cohen's d & Sig. \\\\\\\\\n",
    "\\\\midrule\n",
    "\"\"\"\n",
    "\n",
    "if len(df_ttest) > 0:\n",
    "    for _, row in df_ttest.iterrows():\n",
    "        sig = \"Yes\" if row['significant'] else \"No\"\n",
    "        latex_stats += f\"{row['comparison'].replace('_', ' ')} & {row['metric'].replace('_', ' ')} & {row['mean_diff']:.3f} & {row['t_statistic']:.2f} & {row['p_value']:.4f} & {row['cohens_d']:.2f} & {sig} \\\\\\\\ \\n\"\n",
    "\n",
    "latex_stats += \"\"\"\\\\bottomrule\n",
    "\\\\end{tabular}\n",
    "\\\\end{table}\"\"\"\n",
    "\n",
    "print(latex_stats)\n",
    "\n",
    "# Save\n",
    "with open('../data/results/statistical_tables.tex', 'w') as f:\n",
    "    f.write(latex_stats)\n",
    "print(\"\\nSaved to ../data/results/statistical_tables.tex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Statistical analysis complete!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
