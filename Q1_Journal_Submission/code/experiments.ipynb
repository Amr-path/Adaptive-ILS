{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AILS Comprehensive Experiments\n",
    "\n",
    "This notebook runs comprehensive experiments for the AILS paper:\n",
    "- 100+ scenarios per benchmark map\n",
    "- Multiple grid sizes (50-500)\n",
    "- Multiple obstacle patterns (open, maze, uniform, clustered)\n",
    "- Comparison: Standard A*, AILS-Base, AILS-Adaptive\n",
    "\n",
    "**Author:** Amr Elshahed  \n",
    "**Institution:** Universiti Sains Malaysia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "from AILS_complete import (\n",
    "    AILSPathfinder, AILSConfig,\n",
    "    GridGenerator, MovingAIMapLoader,\n",
    "    run_benchmark, compute_statistics\n",
    ")\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs('../data/results', exist_ok=True)\n",
    "\n",
    "print(\"AILS Experiments Notebook\")\n",
    "print(f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Experiment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "GRID_SIZES = [50, 100, 200, 300, 400, 500]\n",
    "PATTERNS = ['open', 'maze', 'uniform', 'clustered']\n",
    "DENSITIES = [0.1, 0.2, 0.3, 0.4]\n",
    "NUM_PAIRS_PER_CONFIG = 100\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "print(f\"Grid Sizes: {GRID_SIZES}\")\n",
    "print(f\"Patterns: {PATTERNS}\")\n",
    "print(f\"Densities: {DENSITIES}\")\n",
    "print(f\"Pairs per config: {NUM_PAIRS_PER_CONFIG}\")\n",
    "print(f\"Total configurations: {len(GRID_SIZES) * len(PATTERNS) * len(DENSITIES)}\")\n",
    "print(f\"Total path queries: {len(GRID_SIZES) * len(PATTERNS) * len(DENSITIES) * NUM_PAIRS_PER_CONFIG * 3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Grid Generation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_grid(size, pattern, density, seed):\n",
    "    \"\"\"Generate grid based on pattern type.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    if pattern == 'open':\n",
    "        return GridGenerator.generate_random(size, density * 0.5, seed)\n",
    "    elif pattern == 'uniform':\n",
    "        return GridGenerator.generate_random(size, density, seed)\n",
    "    elif pattern == 'clustered':\n",
    "        return GridGenerator.generate_clustered(size, density, max(5, size // 20), seed)\n",
    "    elif pattern == 'maze':\n",
    "        return GridGenerator.generate_maze(size, seed)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown pattern: {pattern}\")\n",
    "\n",
    "# Test grid generation\n",
    "test_grid = generate_grid(100, 'uniform', 0.3, 42)\n",
    "print(f\"Test grid shape: {test_grid.shape}\")\n",
    "print(f\"Obstacle ratio: {np.mean(test_grid):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Synthetic Grid Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_synthetic_experiments():\n",
    "    \"\"\"Run experiments on synthetic grids.\"\"\"\n",
    "    all_results = []\n",
    "    \n",
    "    total_configs = len(GRID_SIZES) * len(PATTERNS) * len(DENSITIES)\n",
    "    pbar = tqdm(total=total_configs, desc=\"Configurations\")\n",
    "    \n",
    "    for size in GRID_SIZES:\n",
    "        for pattern in PATTERNS:\n",
    "            for density in DENSITIES:\n",
    "                # Skip invalid combinations\n",
    "                if pattern == 'maze' and density > 0.1:\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    # Generate grid\n",
    "                    grid = generate_grid(size, pattern, density, RANDOM_SEED)\n",
    "                    \n",
    "                    # Run benchmark\n",
    "                    results = run_benchmark(grid, NUM_PAIRS_PER_CONFIG, RANDOM_SEED)\n",
    "                    \n",
    "                    # Process results\n",
    "                    for method, result_list in results.items():\n",
    "                        for i, r in enumerate(result_list):\n",
    "                            all_results.append({\n",
    "                                'grid_size': size,\n",
    "                                'pattern': pattern,\n",
    "                                'density': density,\n",
    "                                'method': method,\n",
    "                                'pair_id': i,\n",
    "                                'time_ms': r.time_ms,\n",
    "                                'nodes_visited': r.nodes_visited,\n",
    "                                'cost': r.cost,\n",
    "                                'path_found': r.path_found,\n",
    "                                'corridor_size': r.corridor_size,\n",
    "                                'iterations': r.iterations\n",
    "                            })\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    print(f\"Error at {size}x{size} {pattern} d={density}: {e}\")\n",
    "                \n",
    "                pbar.update(1)\n",
    "    \n",
    "    pbar.close()\n",
    "    return pd.DataFrame(all_results)\n",
    "\n",
    "# Run experiments (this may take a while)\n",
    "print(\"Running synthetic grid experiments...\")\n",
    "start_time = time.time()\n",
    "df_synthetic = run_synthetic_experiments()\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\nCompleted in {elapsed/60:.1f} minutes\")\n",
    "print(f\"Total results: {len(df_synthetic)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Moving AI Benchmark Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_movingai_experiments():\n",
    "    \"\"\"Run experiments on Moving AI Lab benchmark maps.\"\"\"\n",
    "    benchmark_maps = [\n",
    "        '../data/benchmark_maps/den312d.map',\n",
    "        '../data/benchmark_maps/ht_chantry.map',\n",
    "        '../data/benchmark_maps/random-64-64-20.map'\n",
    "    ]\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    for map_path in benchmark_maps:\n",
    "        map_name = os.path.basename(map_path).replace('.map', '')\n",
    "        \n",
    "        if not os.path.exists(map_path):\n",
    "            print(f\"Map not found: {map_path}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Processing {map_name}...\")\n",
    "        \n",
    "        try:\n",
    "            # Load map\n",
    "            grid = MovingAIMapLoader.load_map(map_path)\n",
    "            print(f\"  Grid size: {grid.shape}\")\n",
    "            \n",
    "            # Run benchmark\n",
    "            results = run_benchmark(grid, NUM_PAIRS_PER_CONFIG, RANDOM_SEED)\n",
    "            \n",
    "            # Process results\n",
    "            for method, result_list in results.items():\n",
    "                for i, r in enumerate(result_list):\n",
    "                    all_results.append({\n",
    "                        'map_name': map_name,\n",
    "                        'grid_size': max(grid.shape),\n",
    "                        'method': method,\n",
    "                        'pair_id': i,\n",
    "                        'time_ms': r.time_ms,\n",
    "                        'nodes_visited': r.nodes_visited,\n",
    "                        'cost': r.cost,\n",
    "                        'path_found': r.path_found,\n",
    "                        'corridor_size': r.corridor_size,\n",
    "                        'iterations': r.iterations\n",
    "                    })\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"  Error: {e}\")\n",
    "    \n",
    "    return pd.DataFrame(all_results)\n",
    "\n",
    "# Run Moving AI experiments\n",
    "print(\"\\nRunning Moving AI benchmark experiments...\")\n",
    "df_movingai = run_movingai_experiments()\n",
    "print(f\"Total Moving AI results: {len(df_movingai)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_results(df, group_cols):\n",
    "    \"\"\"Generate summary statistics.\"\"\"\n",
    "    summary = df.groupby(group_cols + ['method']).agg({\n",
    "        'time_ms': ['mean', 'std', 'median'],\n",
    "        'nodes_visited': ['mean', 'std', 'median'],\n",
    "        'path_found': 'mean',\n",
    "        'corridor_size': 'mean'\n",
    "    }).round(3)\n",
    "    \n",
    "    summary.columns = ['_'.join(col).strip() for col in summary.columns]\n",
    "    return summary.reset_index()\n",
    "\n",
    "# Synthetic results summary\n",
    "if len(df_synthetic) > 0:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SYNTHETIC GRID RESULTS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    summary_synthetic = summarize_results(df_synthetic, ['grid_size', 'pattern'])\n",
    "    print(summary_synthetic.to_string())\n",
    "\n",
    "# Moving AI results summary  \n",
    "if len(df_movingai) > 0:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MOVING AI BENCHMARK RESULTS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    summary_movingai = summarize_results(df_movingai, ['map_name'])\n",
    "    print(summary_movingai.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "if len(df_synthetic) > 0:\n",
    "    synthetic_path = f'../data/results/synthetic_results_{timestamp}.csv'\n",
    "    df_synthetic.to_csv(synthetic_path, index=False)\n",
    "    print(f\"Saved: {synthetic_path}\")\n",
    "\n",
    "if len(df_movingai) > 0:\n",
    "    movingai_path = f'../data/results/movingai_results_{timestamp}.csv'\n",
    "    df_movingai.to_csv(movingai_path, index=False)\n",
    "    print(f\"Saved: {movingai_path}\")\n",
    "\n",
    "print(\"\\nExperiments complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Quick Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if len(df_synthetic) > 0:\n",
    "    # Filter successful paths\n",
    "    df_success = df_synthetic[df_synthetic['path_found']]\n",
    "    \n",
    "    # Plot time vs grid size\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Time comparison\n",
    "    for method in df_success['method'].unique():\n",
    "        df_m = df_success[df_success['method'] == method]\n",
    "        means = df_m.groupby('grid_size')['time_ms'].mean()\n",
    "        axes[0].plot(means.index, means.values, marker='o', label=method)\n",
    "    \n",
    "    axes[0].set_xlabel('Grid Size')\n",
    "    axes[0].set_ylabel('Time (ms)')\n",
    "    axes[0].set_title('Search Time vs Grid Size')\n",
    "    axes[0].legend()\n",
    "    axes[0].set_yscale('log')\n",
    "    \n",
    "    # Nodes comparison\n",
    "    for method in df_success['method'].unique():\n",
    "        df_m = df_success[df_success['method'] == method]\n",
    "        means = df_m.groupby('grid_size')['nodes_visited'].mean()\n",
    "        axes[1].plot(means.index, means.values, marker='o', label=method)\n",
    "    \n",
    "    axes[1].set_xlabel('Grid Size')\n",
    "    axes[1].set_ylabel('Nodes Visited')\n",
    "    axes[1].set_title('Nodes Visited vs Grid Size')\n",
    "    axes[1].legend()\n",
    "    axes[1].set_yscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../data/results/quick_comparison.png', dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "print(\"Visualization complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
