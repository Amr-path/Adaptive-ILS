{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f200d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "AILS vs MCPP Algorithm Comparison\n",
    "=================================\n",
    "Comprehensive comparison between:\n",
    "1. AILS (Adaptive Incremental Line Search) - from your paper\n",
    "2. MCPP with Graph-Adapted K-Means - from Lee & Lee (2025)\n",
    "\n",
    "This script is designed to run in Jupyter Notebook for academic publication.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfb91e7",
   "metadata": {},
   "source": [
    "# Algorithm Comparison: AILS vs Graph-Adapted K-Means MCPP\n",
    "\n",
    "This notebook compares:\n",
    "- **AILS**: Adaptive Incremental Line Search for single-agent pathfinding optimization\n",
    "- **MCPP-GAK**: Multi-Agent Coverage Path Planning using Graph-Adapted K-Means\n",
    "\n",
    "Both algorithms are evaluated on grid-based environments with varying obstacle densities and patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc75fa80",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a06dffa",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.colors import ListedColormap\n",
    "import heapq\n",
    "from collections import deque, defaultdict\n",
    "import time\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from typing import List, Tuple, Dict, Set, Optional, Callable\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "import random\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import copy\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Configure matplotlib for publication-quality figures\n",
    "plt.rcParams.update({\n",
    "    'figure.figsize': (12, 8),\n",
    "    'font.size': 12,\n",
    "    'axes.labelsize': 14,\n",
    "    'axes.titlesize': 16,\n",
    "    'xtick.labelsize': 12,\n",
    "    'ytick.labelsize': 12,\n",
    "    'legend.fontsize': 11,\n",
    "    'figure.dpi': 150,\n",
    "    'savefig.dpi': 300,\n",
    "    'savefig.bbox': 'tight',\n",
    "    'font.family': 'serif'\n",
    "})\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1a1116",
   "metadata": {},
   "source": [
    "## 2. Grid Map Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39971b7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class ObstaclePattern(Enum):\n",
    "    \"\"\"Obstacle pattern types matching both papers\"\"\"\n",
    "    RANDOM = \"random\"\n",
    "    CLUSTERED = \"clustered\"\n",
    "    MAZE = \"maze\"\n",
    "    ROOMS = \"rooms\"\n",
    "    MIXED = \"mixed\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GridMap:\n",
    "    \"\"\"Grid map representation\"\"\"\n",
    "    grid: np.ndarray\n",
    "    start: Tuple[int, int]\n",
    "    goal: Tuple[int, int]\n",
    "    width: int\n",
    "    height: int\n",
    "    obstacle_density: float\n",
    "    pattern: ObstaclePattern\n",
    "    \n",
    "    def is_valid(self, pos: Tuple[int, int]) -> bool:\n",
    "        \"\"\"Check if position is valid and not an obstacle\"\"\"\n",
    "        x, y = pos\n",
    "        return (0 <= x < self.width and \n",
    "                0 <= y < self.height and \n",
    "                self.grid[y, x] == 0)\n",
    "    \n",
    "    def get_neighbors(self, pos: Tuple[int, int], allow_diagonal: bool = True) -> List[Tuple[int, int]]:\n",
    "        \"\"\"Get valid neighboring positions\"\"\"\n",
    "        x, y = pos\n",
    "        if allow_diagonal:\n",
    "            directions = [(-1,-1), (-1,0), (-1,1), (0,-1), (0,1), (1,-1), (1,0), (1,1)]\n",
    "        else:\n",
    "            directions = [(-1,0), (1,0), (0,-1), (0,1)]\n",
    "        \n",
    "        neighbors = []\n",
    "        for dx, dy in directions:\n",
    "            nx, ny = x + dx, y + dy\n",
    "            if self.is_valid((nx, ny)):\n",
    "                neighbors.append((nx, ny))\n",
    "        return neighbors\n",
    "\n",
    "\n",
    "def generate_random_obstacles(grid: np.ndarray, density: float) -> np.ndarray:\n",
    "    \"\"\"Generate randomly distributed obstacles\"\"\"\n",
    "    height, width = grid.shape\n",
    "    num_obstacles = int(width * height * density)\n",
    "    \n",
    "    positions = np.random.choice(width * height, num_obstacles, replace=False)\n",
    "    for pos in positions:\n",
    "        y, x = divmod(pos, width)\n",
    "        grid[y, x] = 1\n",
    "    return grid\n",
    "\n",
    "\n",
    "def generate_clustered_obstacles(grid: np.ndarray, density: float) -> np.ndarray:\n",
    "    \"\"\"Generate clustered obstacles\"\"\"\n",
    "    height, width = grid.shape\n",
    "    total_obstacles = int(width * height * density)\n",
    "    num_clusters = max(3, int(np.sqrt(total_obstacles)))\n",
    "    obstacles_per_cluster = total_obstacles // num_clusters\n",
    "    \n",
    "    for _ in range(num_clusters):\n",
    "        cx, cy = np.random.randint(0, width), np.random.randint(0, height)\n",
    "        cluster_size = int(np.sqrt(obstacles_per_cluster))\n",
    "        \n",
    "        for _ in range(obstacles_per_cluster):\n",
    "            ox = int(np.clip(cx + np.random.randn() * cluster_size, 0, width-1))\n",
    "            oy = int(np.clip(cy + np.random.randn() * cluster_size, 0, height-1))\n",
    "            grid[oy, ox] = 1\n",
    "    \n",
    "    return grid\n",
    "\n",
    "\n",
    "def generate_maze_obstacles(grid: np.ndarray, density: float) -> np.ndarray:\n",
    "    \"\"\"Generate maze-like obstacles\"\"\"\n",
    "    height, width = grid.shape\n",
    "    wall_spacing = max(3, int(1.0 / np.sqrt(density)))\n",
    "    \n",
    "    # Vertical walls\n",
    "    for x in range(wall_spacing, width, wall_spacing):\n",
    "        gap = np.random.randint(0, height)\n",
    "        for y in range(height):\n",
    "            if abs(y - gap) > 2:\n",
    "                grid[y, x] = 1\n",
    "    \n",
    "    # Horizontal walls\n",
    "    for y in range(wall_spacing, height, wall_spacing):\n",
    "        gap = np.random.randint(0, width)\n",
    "        for x in range(width):\n",
    "            if abs(x - gap) > 2:\n",
    "                grid[y, x] = 1\n",
    "    \n",
    "    return grid\n",
    "\n",
    "\n",
    "def generate_room_obstacles(grid: np.ndarray, density: float) -> np.ndarray:\n",
    "    \"\"\"Generate room-like obstacles with doors\"\"\"\n",
    "    height, width = grid.shape\n",
    "    room_size = max(5, int(np.sqrt(width * height) / 4))\n",
    "    \n",
    "    for room_y in range(0, height, room_size):\n",
    "        for room_x in range(0, width, room_size):\n",
    "            # Room walls\n",
    "            for x in range(room_x, min(room_x + room_size, width)):\n",
    "                if room_y + room_size - 1 < height:\n",
    "                    grid[room_y + room_size - 1, x] = 1\n",
    "            for y in range(room_y, min(room_y + room_size, height)):\n",
    "                if room_x + room_size - 1 < width:\n",
    "                    grid[y, room_x + room_size - 1] = 1\n",
    "            \n",
    "            # Create doors\n",
    "            if room_x + room_size - 1 < width:\n",
    "                door_y = room_y + room_size // 2\n",
    "                if door_y < height:\n",
    "                    grid[door_y, room_x + room_size - 1] = 0\n",
    "            if room_y + room_size - 1 < height:\n",
    "                door_x = room_x + room_size // 2\n",
    "                if door_x < width:\n",
    "                    grid[room_y + room_size - 1, door_x] = 0\n",
    "    \n",
    "    return grid\n",
    "\n",
    "\n",
    "def generate_mixed_obstacles(grid: np.ndarray, density: float) -> np.ndarray:\n",
    "    \"\"\"Generate mixed obstacle patterns\"\"\"\n",
    "    height, width = grid.shape\n",
    "    \n",
    "    # Divide into quadrants with different patterns\n",
    "    half_h, half_w = height // 2, width // 2\n",
    "    \n",
    "    # Random in top-left\n",
    "    generate_random_obstacles(grid[:half_h, :half_w], density)\n",
    "    \n",
    "    # Clustered in top-right\n",
    "    generate_clustered_obstacles(grid[:half_h, half_w:], density)\n",
    "    \n",
    "    # Some walls\n",
    "    for y in range(half_h, height):\n",
    "        for x in range(0, half_w):\n",
    "            if np.random.random() < density * 0.5:\n",
    "                grid[y, x] = 1\n",
    "    \n",
    "    # Random in bottom-right\n",
    "    generate_random_obstacles(grid[half_h:, half_w:], density * 0.8)\n",
    "    \n",
    "    return grid\n",
    "\n",
    "\n",
    "def generate_grid_map(width: int, height: int, \n",
    "                      obstacle_density: float,\n",
    "                      pattern: ObstaclePattern,\n",
    "                      ensure_path: bool = True) -> GridMap:\n",
    "    \"\"\"Generate a grid map with specified parameters\"\"\"\n",
    "    \n",
    "    grid = np.zeros((height, width), dtype=np.int32)\n",
    "    \n",
    "    # Generate obstacles based on pattern\n",
    "    generators = {\n",
    "        ObstaclePattern.RANDOM: generate_random_obstacles,\n",
    "        ObstaclePattern.CLUSTERED: generate_clustered_obstacles,\n",
    "        ObstaclePattern.MAZE: generate_maze_obstacles,\n",
    "        ObstaclePattern.ROOMS: generate_room_obstacles,\n",
    "        ObstaclePattern.MIXED: generate_mixed_obstacles\n",
    "    }\n",
    "    \n",
    "    grid = generators[pattern](grid, obstacle_density)\n",
    "    \n",
    "    # Set start and goal positions\n",
    "    def find_free_position(grid, prefer_corner=None):\n",
    "        height, width = grid.shape\n",
    "        if prefer_corner == 'topleft':\n",
    "            for y in range(height // 4):\n",
    "                for x in range(width // 4):\n",
    "                    if grid[y, x] == 0:\n",
    "                        return (x, y)\n",
    "        elif prefer_corner == 'bottomright':\n",
    "            for y in range(height - 1, 3 * height // 4, -1):\n",
    "                for x in range(width - 1, 3 * width // 4, -1):\n",
    "                    if grid[y, x] == 0:\n",
    "                        return (x, y)\n",
    "        \n",
    "        # Fallback: find any free position\n",
    "        free_positions = np.argwhere(grid == 0)\n",
    "        if len(free_positions) > 0:\n",
    "            idx = np.random.randint(len(free_positions))\n",
    "            return (free_positions[idx][1], free_positions[idx][0])\n",
    "        return None\n",
    "    \n",
    "    start = find_free_position(grid, 'topleft')\n",
    "    goal = find_free_position(grid, 'bottomright')\n",
    "    \n",
    "    if start is None or goal is None:\n",
    "        # Regenerate with lower density\n",
    "        return generate_grid_map(width, height, obstacle_density * 0.8, pattern, ensure_path)\n",
    "    \n",
    "    # Ensure start and goal are free\n",
    "    grid[start[1], start[0]] = 0\n",
    "    grid[goal[1], goal[0]] = 0\n",
    "    \n",
    "    return GridMap(\n",
    "        grid=grid,\n",
    "        start=start,\n",
    "        goal=goal,\n",
    "        width=width,\n",
    "        height=height,\n",
    "        obstacle_density=obstacle_density,\n",
    "        pattern=pattern\n",
    "    )\n",
    "\n",
    "\n",
    "def visualize_grid(grid_map: GridMap, path: List[Tuple[int, int]] = None,\n",
    "                   corridor: Set[Tuple[int, int]] = None,\n",
    "                   title: str = \"\", ax=None):\n",
    "    \"\"\"Visualize a grid map with optional path and corridor\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    \n",
    "    # Create visualization grid\n",
    "    vis_grid = np.zeros((grid_map.height, grid_map.width, 3))\n",
    "    \n",
    "    # Background (white)\n",
    "    vis_grid[:, :] = [1, 1, 1]\n",
    "    \n",
    "    # Obstacles (black)\n",
    "    vis_grid[grid_map.grid == 1] = [0.2, 0.2, 0.2]\n",
    "    \n",
    "    # Corridor (light green)\n",
    "    if corridor:\n",
    "        for x, y in corridor:\n",
    "            if 0 <= x < grid_map.width and 0 <= y < grid_map.height:\n",
    "                if grid_map.grid[y, x] == 0:\n",
    "                    vis_grid[y, x] = [0.7, 0.9, 0.7]\n",
    "    \n",
    "    # Path (blue)\n",
    "    if path:\n",
    "        for x, y in path:\n",
    "            vis_grid[y, x] = [0.2, 0.4, 0.8]\n",
    "    \n",
    "    # Start (green) and Goal (red)\n",
    "    vis_grid[grid_map.start[1], grid_map.start[0]] = [0, 0.8, 0]\n",
    "    vis_grid[grid_map.goal[1], grid_map.goal[0]] = [0.8, 0, 0]\n",
    "    \n",
    "    ax.imshow(vis_grid, origin='upper')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    \n",
    "    # Legend\n",
    "    legend_elements = [\n",
    "        mpatches.Patch(facecolor=[0, 0.8, 0], label='Start'),\n",
    "        mpatches.Patch(facecolor=[0.8, 0, 0], label='Goal'),\n",
    "        mpatches.Patch(facecolor=[0.2, 0.2, 0.2], label='Obstacle'),\n",
    "    ]\n",
    "    if path:\n",
    "        legend_elements.append(mpatches.Patch(facecolor=[0.2, 0.4, 0.8], label='Path'))\n",
    "    if corridor:\n",
    "        legend_elements.append(mpatches.Patch(facecolor=[0.7, 0.9, 0.7], label='Corridor'))\n",
    "    \n",
    "    ax.legend(handles=legend_elements, loc='upper right')\n",
    "    \n",
    "    return ax\n",
    "\n",
    "\n",
    "print(\"Grid generation functions created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e2d503",
   "metadata": {},
   "source": [
    "## 3. AILS Algorithm Implementation (From Your Paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b56df2f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class AILSPathfinder:\n",
    "    \"\"\"\n",
    "    Adaptive Incremental Line Search (AILS) Implementation\n",
    "    Based on the paper: \"Adaptive Incremental Line Search: A Dynamic Corridor-Based \n",
    "    Optimization Framework for Grid-Based Pathfinding\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, r_min: int = 1, r_max: int = 5, \n",
    "                 window_size: int = 7, alpha: float = 1.0):\n",
    "        \"\"\"\n",
    "        Initialize AILS pathfinder\n",
    "        \n",
    "        Args:\n",
    "            r_min: Minimum corridor radius\n",
    "            r_max: Maximum corridor radius\n",
    "            window_size: Window size for density estimation\n",
    "            alpha: Exponent for density-based radius adjustment\n",
    "        \"\"\"\n",
    "        self.r_min = r_min\n",
    "        self.r_max = r_max\n",
    "        self.window_size = window_size\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def bresenham_line(self, start: Tuple[int, int], \n",
    "                       end: Tuple[int, int]) -> List[Tuple[int, int]]:\n",
    "        \"\"\"Generate points along Bresenham's line\"\"\"\n",
    "        x0, y0 = start\n",
    "        x1, y1 = end\n",
    "        \n",
    "        points = []\n",
    "        dx = abs(x1 - x0)\n",
    "        dy = abs(y1 - y0)\n",
    "        sx = 1 if x0 < x1 else -1\n",
    "        sy = 1 if y0 < y1 else -1\n",
    "        err = dx - dy\n",
    "        \n",
    "        while True:\n",
    "            points.append((x0, y0))\n",
    "            if x0 == x1 and y0 == y1:\n",
    "                break\n",
    "            e2 = 2 * err\n",
    "            if e2 > -dy:\n",
    "                err -= dy\n",
    "                x0 += sx\n",
    "            if e2 < dx:\n",
    "                err += dx\n",
    "                y0 += sy\n",
    "        \n",
    "        return points\n",
    "    \n",
    "    def compute_local_density(self, grid_map: GridMap, \n",
    "                              point: Tuple[int, int]) -> float:\n",
    "        \"\"\"Compute local obstacle density around a point\"\"\"\n",
    "        x, y = point\n",
    "        half_w = self.window_size // 2\n",
    "        \n",
    "        obstacle_count = 0\n",
    "        total_count = 0\n",
    "        \n",
    "        for dy in range(-half_w, half_w + 1):\n",
    "            for dx in range(-half_w, half_w + 1):\n",
    "                nx, ny = x + dx, y + dy\n",
    "                if 0 <= nx < grid_map.width and 0 <= ny < grid_map.height:\n",
    "                    total_count += 1\n",
    "                    if grid_map.grid[ny, nx] == 1:\n",
    "                        obstacle_count += 1\n",
    "        \n",
    "        return obstacle_count / max(total_count, 1)\n",
    "    \n",
    "    def compute_density_field(self, grid_map: GridMap,\n",
    "                              line_points: List[Tuple[int, int]]) -> Dict[Tuple[int, int], float]:\n",
    "        \"\"\"Compute density field along the Bresenham line\"\"\"\n",
    "        density_field = {}\n",
    "        for point in line_points:\n",
    "            density_field[point] = self.compute_local_density(grid_map, point)\n",
    "        return density_field\n",
    "    \n",
    "    def compute_adaptive_radius(self, density: float) -> int:\n",
    "        \"\"\"Compute adaptive corridor radius based on local density\"\"\"\n",
    "        radius = self.r_min + int((self.r_max - self.r_min) * (density ** self.alpha))\n",
    "        return min(radius, self.r_max)\n",
    "    \n",
    "    def build_adaptive_corridor(self, grid_map: GridMap,\n",
    "                                 line_points: List[Tuple[int, int]],\n",
    "                                 density_field: Dict[Tuple[int, int], float]) -> Set[Tuple[int, int]]:\n",
    "        \"\"\"Build adaptive corridor around Bresenham line\"\"\"\n",
    "        corridor = set()\n",
    "        \n",
    "        for point in line_points:\n",
    "            density = density_field.get(point, 0)\n",
    "            radius = self.compute_adaptive_radius(density)\n",
    "            \n",
    "            # Add all points within radius\n",
    "            x, y = point\n",
    "            for dy in range(-radius, radius + 1):\n",
    "                for dx in range(-radius, radius + 1):\n",
    "                    nx, ny = x + dx, y + dy\n",
    "                    if (0 <= nx < grid_map.width and \n",
    "                        0 <= ny < grid_map.height and\n",
    "                        dx*dx + dy*dy <= radius*radius):\n",
    "                        corridor.add((nx, ny))\n",
    "        \n",
    "        return corridor\n",
    "    \n",
    "    def expand_corridor(self, grid_map: GridMap, \n",
    "                        corridor: Set[Tuple[int, int]],\n",
    "                        expansion: int = 2) -> Set[Tuple[int, int]]:\n",
    "        \"\"\"Expand corridor when path not found\"\"\"\n",
    "        expanded = set(corridor)\n",
    "        \n",
    "        for _ in range(expansion):\n",
    "            new_cells = set()\n",
    "            for x, y in expanded:\n",
    "                for dx, dy in [(-1,0), (1,0), (0,-1), (0,1)]:\n",
    "                    nx, ny = x + dx, y + dy\n",
    "                    if (0 <= nx < grid_map.width and \n",
    "                        0 <= ny < grid_map.height):\n",
    "                        new_cells.add((nx, ny))\n",
    "            expanded.update(new_cells)\n",
    "        \n",
    "        return expanded\n",
    "    \n",
    "    def find_path_in_corridor(self, grid_map: GridMap,\n",
    "                               corridor: Set[Tuple[int, int]],\n",
    "                               algorithm: str = 'astar') -> Tuple[List[Tuple[int, int]], int, float]:\n",
    "        \"\"\"\n",
    "        Find path within corridor using specified algorithm\n",
    "        \n",
    "        Returns:\n",
    "            path: List of positions\n",
    "            visited_nodes: Number of nodes visited\n",
    "            execution_time: Time taken in seconds\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        visited_count = 0\n",
    "        \n",
    "        if algorithm == 'astar':\n",
    "            path, visited_count = self._astar_corridor(grid_map, corridor)\n",
    "        elif algorithm == 'dijkstra':\n",
    "            path, visited_count = self._dijkstra_corridor(grid_map, corridor)\n",
    "        elif algorithm == 'bfs':\n",
    "            path, visited_count = self._bfs_corridor(grid_map, corridor)\n",
    "        elif algorithm == 'dfs':\n",
    "            path, visited_count = self._dfs_corridor(grid_map, corridor)\n",
    "        elif algorithm == 'greedy':\n",
    "            path, visited_count = self._greedy_corridor(grid_map, corridor)\n",
    "        elif algorithm == 'bidirectional':\n",
    "            path, visited_count = self._bidirectional_corridor(grid_map, corridor)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown algorithm: {algorithm}\")\n",
    "        \n",
    "        execution_time = time.time() - start_time\n",
    "        return path, visited_count, execution_time\n",
    "    \n",
    "    def _heuristic(self, a: Tuple[int, int], b: Tuple[int, int]) -> float:\n",
    "        \"\"\"Manhattan distance heuristic\"\"\"\n",
    "        return abs(a[0] - b[0]) + abs(a[1] - b[1])\n",
    "    \n",
    "    def _astar_corridor(self, grid_map: GridMap, \n",
    "                        corridor: Set[Tuple[int, int]]) -> Tuple[List[Tuple[int, int]], int]:\n",
    "        \"\"\"A* search within corridor\"\"\"\n",
    "        start, goal = grid_map.start, grid_map.goal\n",
    "        \n",
    "        open_set = [(0, start)]\n",
    "        came_from = {}\n",
    "        g_score = {start: 0}\n",
    "        f_score = {start: self._heuristic(start, goal)}\n",
    "        visited = set()\n",
    "        \n",
    "        while open_set:\n",
    "            _, current = heapq.heappop(open_set)\n",
    "            \n",
    "            if current in visited:\n",
    "                continue\n",
    "            visited.add(current)\n",
    "            \n",
    "            if current == goal:\n",
    "                # Reconstruct path\n",
    "                path = [current]\n",
    "                while current in came_from:\n",
    "                    current = came_from[current]\n",
    "                    path.append(current)\n",
    "                return path[::-1], len(visited)\n",
    "            \n",
    "            for neighbor in grid_map.get_neighbors(current):\n",
    "                if neighbor not in corridor or neighbor in visited:\n",
    "                    continue\n",
    "                \n",
    "                tentative_g = g_score[current] + 1\n",
    "                \n",
    "                if neighbor not in g_score or tentative_g < g_score[neighbor]:\n",
    "                    came_from[neighbor] = current\n",
    "                    g_score[neighbor] = tentative_g\n",
    "                    f_score[neighbor] = tentative_g + self._heuristic(neighbor, goal)\n",
    "                    heapq.heappush(open_set, (f_score[neighbor], neighbor))\n",
    "        \n",
    "        return [], len(visited)\n",
    "    \n",
    "    def _dijkstra_corridor(self, grid_map: GridMap,\n",
    "                           corridor: Set[Tuple[int, int]]) -> Tuple[List[Tuple[int, int]], int]:\n",
    "        \"\"\"Dijkstra's algorithm within corridor\"\"\"\n",
    "        start, goal = grid_map.start, grid_map.goal\n",
    "        \n",
    "        dist = {start: 0}\n",
    "        prev = {}\n",
    "        pq = [(0, start)]\n",
    "        visited = set()\n",
    "        \n",
    "        while pq:\n",
    "            d, current = heapq.heappop(pq)\n",
    "            \n",
    "            if current in visited:\n",
    "                continue\n",
    "            visited.add(current)\n",
    "            \n",
    "            if current == goal:\n",
    "                path = [current]\n",
    "                while current in prev:\n",
    "                    current = prev[current]\n",
    "                    path.append(current)\n",
    "                return path[::-1], len(visited)\n",
    "            \n",
    "            for neighbor in grid_map.get_neighbors(current):\n",
    "                if neighbor not in corridor or neighbor in visited:\n",
    "                    continue\n",
    "                \n",
    "                new_dist = dist[current] + 1\n",
    "                if neighbor not in dist or new_dist < dist[neighbor]:\n",
    "                    dist[neighbor] = new_dist\n",
    "                    prev[neighbor] = current\n",
    "                    heapq.heappush(pq, (new_dist, neighbor))\n",
    "        \n",
    "        return [], len(visited)\n",
    "    \n",
    "    def _bfs_corridor(self, grid_map: GridMap,\n",
    "                      corridor: Set[Tuple[int, int]]) -> Tuple[List[Tuple[int, int]], int]:\n",
    "        \"\"\"BFS within corridor\"\"\"\n",
    "        start, goal = grid_map.start, grid_map.goal\n",
    "        \n",
    "        queue = deque([start])\n",
    "        visited = {start}\n",
    "        prev = {}\n",
    "        \n",
    "        while queue:\n",
    "            current = queue.popleft()\n",
    "            \n",
    "            if current == goal:\n",
    "                path = [current]\n",
    "                while current in prev:\n",
    "                    current = prev[current]\n",
    "                    path.append(current)\n",
    "                return path[::-1], len(visited)\n",
    "            \n",
    "            for neighbor in grid_map.get_neighbors(current, allow_diagonal=False):\n",
    "                if neighbor in corridor and neighbor not in visited:\n",
    "                    visited.add(neighbor)\n",
    "                    prev[neighbor] = current\n",
    "                    queue.append(neighbor)\n",
    "        \n",
    "        return [], len(visited)\n",
    "    \n",
    "    def _dfs_corridor(self, grid_map: GridMap,\n",
    "                      corridor: Set[Tuple[int, int]]) -> Tuple[List[Tuple[int, int]], int]:\n",
    "        \"\"\"DFS within corridor\"\"\"\n",
    "        start, goal = grid_map.start, grid_map.goal\n",
    "        \n",
    "        stack = [start]\n",
    "        visited = set()\n",
    "        prev = {}\n",
    "        \n",
    "        while stack:\n",
    "            current = stack.pop()\n",
    "            \n",
    "            if current in visited:\n",
    "                continue\n",
    "            visited.add(current)\n",
    "            \n",
    "            if current == goal:\n",
    "                path = [current]\n",
    "                while current in prev:\n",
    "                    current = prev[current]\n",
    "                    path.append(current)\n",
    "                return path[::-1], len(visited)\n",
    "            \n",
    "            for neighbor in grid_map.get_neighbors(current, allow_diagonal=False):\n",
    "                if neighbor in corridor and neighbor not in visited:\n",
    "                    prev[neighbor] = current\n",
    "                    stack.append(neighbor)\n",
    "        \n",
    "        return [], len(visited)\n",
    "    \n",
    "    def _greedy_corridor(self, grid_map: GridMap,\n",
    "                         corridor: Set[Tuple[int, int]]) -> Tuple[List[Tuple[int, int]], int]:\n",
    "        \"\"\"Greedy best-first search within corridor\"\"\"\n",
    "        start, goal = grid_map.start, grid_map.goal\n",
    "        \n",
    "        open_set = [(self._heuristic(start, goal), start)]\n",
    "        came_from = {}\n",
    "        visited = set()\n",
    "        \n",
    "        while open_set:\n",
    "            _, current = heapq.heappop(open_set)\n",
    "            \n",
    "            if current in visited:\n",
    "                continue\n",
    "            visited.add(current)\n",
    "            \n",
    "            if current == goal:\n",
    "                path = [current]\n",
    "                while current in came_from:\n",
    "                    current = came_from[current]\n",
    "                    path.append(current)\n",
    "                return path[::-1], len(visited)\n",
    "            \n",
    "            for neighbor in grid_map.get_neighbors(current):\n",
    "                if neighbor in corridor and neighbor not in visited:\n",
    "                    came_from[neighbor] = current\n",
    "                    heapq.heappush(open_set, (self._heuristic(neighbor, goal), neighbor))\n",
    "        \n",
    "        return [], len(visited)\n",
    "    \n",
    "    def _bidirectional_corridor(self, grid_map: GridMap,\n",
    "                                 corridor: Set[Tuple[int, int]]) -> Tuple[List[Tuple[int, int]], int]:\n",
    "        \"\"\"Bidirectional BFS within corridor\"\"\"\n",
    "        start, goal = grid_map.start, grid_map.goal\n",
    "        \n",
    "        front_queue = deque([start])\n",
    "        back_queue = deque([goal])\n",
    "        \n",
    "        front_visited = {start: None}\n",
    "        back_visited = {goal: None}\n",
    "        \n",
    "        meeting_point = None\n",
    "        total_visited = 0\n",
    "        \n",
    "        while front_queue and back_queue:\n",
    "            # Expand from start\n",
    "            if front_queue:\n",
    "                current = front_queue.popleft()\n",
    "                total_visited += 1\n",
    "                \n",
    "                if current in back_visited:\n",
    "                    meeting_point = current\n",
    "                    break\n",
    "                \n",
    "                for neighbor in grid_map.get_neighbors(current, allow_diagonal=False):\n",
    "                    if neighbor in corridor and neighbor not in front_visited:\n",
    "                        front_visited[neighbor] = current\n",
    "                        front_queue.append(neighbor)\n",
    "            \n",
    "            # Expand from goal\n",
    "            if back_queue:\n",
    "                current = back_queue.popleft()\n",
    "                total_visited += 1\n",
    "                \n",
    "                if current in front_visited:\n",
    "                    meeting_point = current\n",
    "                    break\n",
    "                \n",
    "                for neighbor in grid_map.get_neighbors(current, allow_diagonal=False):\n",
    "                    if neighbor in corridor and neighbor not in back_visited:\n",
    "                        back_visited[neighbor] = current\n",
    "                        back_queue.append(neighbor)\n",
    "        \n",
    "        if meeting_point is None:\n",
    "            return [], total_visited\n",
    "        \n",
    "        # Reconstruct path\n",
    "        path = []\n",
    "        current = meeting_point\n",
    "        while current is not None:\n",
    "            path.append(current)\n",
    "            current = front_visited.get(current)\n",
    "        path = path[::-1]\n",
    "        \n",
    "        current = back_visited.get(meeting_point)\n",
    "        while current is not None:\n",
    "            path.append(current)\n",
    "            current = back_visited.get(current)\n",
    "        \n",
    "        return path, total_visited\n",
    "    \n",
    "    def find_path(self, grid_map: GridMap, \n",
    "                  algorithm: str = 'astar') -> Dict:\n",
    "        \"\"\"\n",
    "        Main AILS pathfinding method\n",
    "        \n",
    "        Returns dict with path, metrics, and corridor info\n",
    "        \"\"\"\n",
    "        # Build Bresenham line\n",
    "        line_points = self.bresenham_line(grid_map.start, grid_map.goal)\n",
    "        \n",
    "        # Compute density field\n",
    "        density_field = self.compute_density_field(grid_map, line_points)\n",
    "        \n",
    "        # Build adaptive corridor\n",
    "        corridor = self.build_adaptive_corridor(grid_map, line_points, density_field)\n",
    "        \n",
    "        # Add start and goal to corridor\n",
    "        corridor.add(grid_map.start)\n",
    "        corridor.add(grid_map.goal)\n",
    "        \n",
    "        # Find path within corridor\n",
    "        path, visited, exec_time = self.find_path_in_corridor(grid_map, corridor, algorithm)\n",
    "        \n",
    "        # If no path found, expand corridor\n",
    "        max_expansions = 3\n",
    "        expansion = 0\n",
    "        while not path and expansion < max_expansions:\n",
    "            corridor = self.expand_corridor(grid_map, corridor)\n",
    "            path, visited, exec_time = self.find_path_in_corridor(grid_map, corridor, algorithm)\n",
    "            expansion += 1\n",
    "        \n",
    "        return {\n",
    "            'path': path,\n",
    "            'path_length': len(path) if path else -1,\n",
    "            'visited_nodes': visited,\n",
    "            'execution_time': exec_time,\n",
    "            'corridor_size': len(corridor),\n",
    "            'corridor': corridor,\n",
    "            'expansions': expansion\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"AILS Pathfinder implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923db21c",
   "metadata": {},
   "source": [
    "## 4. Standard Pathfinding Algorithms (Baselines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77d676b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class StandardPathfinder:\n",
    "    \"\"\"Standard pathfinding algorithms without corridor optimization\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def _heuristic(self, a: Tuple[int, int], b: Tuple[int, int]) -> float:\n",
    "        return abs(a[0] - b[0]) + abs(a[1] - b[1])\n",
    "    \n",
    "    def astar(self, grid_map: GridMap) -> Dict:\n",
    "        \"\"\"Standard A* algorithm\"\"\"\n",
    "        start_time = time.time()\n",
    "        start, goal = grid_map.start, grid_map.goal\n",
    "        \n",
    "        open_set = [(0, start)]\n",
    "        came_from = {}\n",
    "        g_score = {start: 0}\n",
    "        visited = set()\n",
    "        \n",
    "        while open_set:\n",
    "            _, current = heapq.heappop(open_set)\n",
    "            \n",
    "            if current in visited:\n",
    "                continue\n",
    "            visited.add(current)\n",
    "            \n",
    "            if current == goal:\n",
    "                path = [current]\n",
    "                while current in came_from:\n",
    "                    current = came_from[current]\n",
    "                    path.append(current)\n",
    "                path = path[::-1]\n",
    "                return {\n",
    "                    'path': path,\n",
    "                    'path_length': len(path),\n",
    "                    'visited_nodes': len(visited),\n",
    "                    'execution_time': time.time() - start_time\n",
    "                }\n",
    "            \n",
    "            for neighbor in grid_map.get_neighbors(current):\n",
    "                if neighbor in visited:\n",
    "                    continue\n",
    "                \n",
    "                tentative_g = g_score[current] + 1\n",
    "                \n",
    "                if neighbor not in g_score or tentative_g < g_score[neighbor]:\n",
    "                    came_from[neighbor] = current\n",
    "                    g_score[neighbor] = tentative_g\n",
    "                    f = tentative_g + self._heuristic(neighbor, goal)\n",
    "                    heapq.heappush(open_set, (f, neighbor))\n",
    "        \n",
    "        return {\n",
    "            'path': [],\n",
    "            'path_length': -1,\n",
    "            'visited_nodes': len(visited),\n",
    "            'execution_time': time.time() - start_time\n",
    "        }\n",
    "    \n",
    "    def dijkstra(self, grid_map: GridMap) -> Dict:\n",
    "        \"\"\"Standard Dijkstra's algorithm\"\"\"\n",
    "        start_time = time.time()\n",
    "        start, goal = grid_map.start, grid_map.goal\n",
    "        \n",
    "        dist = {start: 0}\n",
    "        prev = {}\n",
    "        pq = [(0, start)]\n",
    "        visited = set()\n",
    "        \n",
    "        while pq:\n",
    "            d, current = heapq.heappop(pq)\n",
    "            \n",
    "            if current in visited:\n",
    "                continue\n",
    "            visited.add(current)\n",
    "            \n",
    "            if current == goal:\n",
    "                path = [current]\n",
    "                while current in prev:\n",
    "                    current = prev[current]\n",
    "                    path.append(current)\n",
    "                path = path[::-1]\n",
    "                return {\n",
    "                    'path': path,\n",
    "                    'path_length': len(path),\n",
    "                    'visited_nodes': len(visited),\n",
    "                    'execution_time': time.time() - start_time\n",
    "                }\n",
    "            \n",
    "            for neighbor in grid_map.get_neighbors(current):\n",
    "                if neighbor in visited:\n",
    "                    continue\n",
    "                \n",
    "                new_dist = dist[current] + 1\n",
    "                if neighbor not in dist or new_dist < dist[neighbor]:\n",
    "                    dist[neighbor] = new_dist\n",
    "                    prev[neighbor] = current\n",
    "                    heapq.heappush(pq, (new_dist, neighbor))\n",
    "        \n",
    "        return {\n",
    "            'path': [],\n",
    "            'path_length': -1,\n",
    "            'visited_nodes': len(visited),\n",
    "            'execution_time': time.time() - start_time\n",
    "        }\n",
    "    \n",
    "    def bfs(self, grid_map: GridMap) -> Dict:\n",
    "        \"\"\"Standard BFS\"\"\"\n",
    "        start_time = time.time()\n",
    "        start, goal = grid_map.start, grid_map.goal\n",
    "        \n",
    "        queue = deque([start])\n",
    "        visited = {start}\n",
    "        prev = {}\n",
    "        \n",
    "        while queue:\n",
    "            current = queue.popleft()\n",
    "            \n",
    "            if current == goal:\n",
    "                path = [current]\n",
    "                while current in prev:\n",
    "                    current = prev[current]\n",
    "                    path.append(current)\n",
    "                path = path[::-1]\n",
    "                return {\n",
    "                    'path': path,\n",
    "                    'path_length': len(path),\n",
    "                    'visited_nodes': len(visited),\n",
    "                    'execution_time': time.time() - start_time\n",
    "                }\n",
    "            \n",
    "            for neighbor in grid_map.get_neighbors(current, allow_diagonal=False):\n",
    "                if neighbor not in visited:\n",
    "                    visited.add(neighbor)\n",
    "                    prev[neighbor] = current\n",
    "                    queue.append(neighbor)\n",
    "        \n",
    "        return {\n",
    "            'path': [],\n",
    "            'path_length': -1,\n",
    "            'visited_nodes': len(visited),\n",
    "            'execution_time': time.time() - start_time\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"Standard pathfinders implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc5f4d1",
   "metadata": {},
   "source": [
    "## 5. MCPP with Graph-Adapted K-Means (From Lee & Lee Paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cb4c8d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class MCPPGraphKMeans:\n",
    "    \"\"\"\n",
    "    Multi-Agent Coverage Path Planning using Graph-Adapted K-Means\n",
    "    Based on Lee & Lee (2025) paper\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_agents: int = 2, max_iter: int = 100, threshold: float = 0.005):\n",
    "        \"\"\"\n",
    "        Initialize MCPP solver\n",
    "        \n",
    "        Args:\n",
    "            num_agents: Number of agents (k)\n",
    "            max_iter: Maximum iterations for clustering\n",
    "            threshold: Convergence threshold\n",
    "        \"\"\"\n",
    "        self.k = num_agents\n",
    "        self.max_iter = max_iter\n",
    "        self.threshold = threshold\n",
    "        \n",
    "    def grid_to_graph(self, grid_map: GridMap) -> Tuple[List[Tuple[int, int]], Dict]:\n",
    "        \"\"\"Convert grid map to graph representation\"\"\"\n",
    "        vertices = []\n",
    "        edges = defaultdict(list)\n",
    "        \n",
    "        for y in range(grid_map.height):\n",
    "            for x in range(grid_map.width):\n",
    "                if grid_map.grid[y, x] == 0:\n",
    "                    vertices.append((x, y))\n",
    "        \n",
    "        vertex_set = set(vertices)\n",
    "        \n",
    "        for v in vertices:\n",
    "            for neighbor in grid_map.get_neighbors(v, allow_diagonal=False):\n",
    "                if neighbor in vertex_set:\n",
    "                    edges[v].append(neighbor)\n",
    "        \n",
    "        return vertices, dict(edges)\n",
    "    \n",
    "    def compute_shortest_paths(self, vertices: List[Tuple[int, int]], \n",
    "                                edges: Dict) -> Dict[Tuple, int]:\n",
    "        \"\"\"Compute shortest path distances between vertices using BFS\"\"\"\n",
    "        distances = {}\n",
    "        \n",
    "        for start in vertices:\n",
    "            dist = {start: 0}\n",
    "            queue = deque([start])\n",
    "            \n",
    "            while queue:\n",
    "                current = queue.popleft()\n",
    "                \n",
    "                for neighbor in edges.get(current, []):\n",
    "                    if neighbor not in dist:\n",
    "                        dist[neighbor] = dist[current] + 1\n",
    "                        queue.append(neighbor)\n",
    "            \n",
    "            for end in vertices:\n",
    "                if end in dist:\n",
    "                    distances[(start, end)] = dist[end]\n",
    "                else:\n",
    "                    distances[(start, end)] = float('inf')\n",
    "        \n",
    "        return distances\n",
    "    \n",
    "    def compute_internal_weight(self, cluster: Set[Tuple[int, int]], \n",
    "                                 edges: Dict) -> int:\n",
    "        \"\"\"Compute sum of internal edge weights for a cluster\"\"\"\n",
    "        weight = 0\n",
    "        for v in cluster:\n",
    "            for neighbor in edges.get(v, []):\n",
    "                if neighbor in cluster:\n",
    "                    weight += 1\n",
    "        return weight // 2  # Each edge counted twice\n",
    "    \n",
    "    def initialize_clusters(self, vertices: List[Tuple[int, int]],\n",
    "                            edges: Dict,\n",
    "                            agent_positions: List[Tuple[int, int]]) -> List[Set[Tuple[int, int]]]:\n",
    "        \"\"\"Initialize clusters using region growing from agent positions\"\"\"\n",
    "        clusters = [set() for _ in range(self.k)]\n",
    "        assigned = set()\n",
    "        target_size = len(vertices) // self.k\n",
    "        \n",
    "        # Region growing from each agent's position\n",
    "        for i, start in enumerate(agent_positions):\n",
    "            if start not in set(vertices):\n",
    "                # Find nearest vertex\n",
    "                min_dist = float('inf')\n",
    "                for v in vertices:\n",
    "                    d = abs(v[0] - start[0]) + abs(v[1] - start[1])\n",
    "                    if d < min_dist and v not in assigned:\n",
    "                        min_dist = d\n",
    "                        start = v\n",
    "            \n",
    "            queue = deque([start])\n",
    "            while queue and len(clusters[i]) < target_size * 0.8:\n",
    "                current = queue.popleft()\n",
    "                if current in assigned:\n",
    "                    continue\n",
    "                \n",
    "                clusters[i].add(current)\n",
    "                assigned.add(current)\n",
    "                \n",
    "                for neighbor in edges.get(current, []):\n",
    "                    if neighbor not in assigned:\n",
    "                        queue.append(neighbor)\n",
    "        \n",
    "        # Assign remaining vertices\n",
    "        remaining = set(vertices) - assigned\n",
    "        for v in remaining:\n",
    "            # Assign to nearest cluster\n",
    "            min_dist = float('inf')\n",
    "            best_cluster = 0\n",
    "            for i, cluster in enumerate(clusters):\n",
    "                if len(cluster) > 0:\n",
    "                    for cv in cluster:\n",
    "                        d = abs(v[0] - cv[0]) + abs(v[1] - cv[1])\n",
    "                        if d < min_dist:\n",
    "                            min_dist = d\n",
    "                            best_cluster = i\n",
    "            clusters[best_cluster].add(v)\n",
    "        \n",
    "        return clusters\n",
    "    \n",
    "    def compute_target_ratios(self, clusters: List[Set[Tuple[int, int]]],\n",
    "                               agent_positions: List[Tuple[int, int]],\n",
    "                               edges: Dict) -> List[float]:\n",
    "        \"\"\"Compute target ratio R_i for each cluster\"\"\"\n",
    "        total_vertices = sum(len(c) for c in clusters)\n",
    "        \n",
    "        # Compute initial path distances\n",
    "        init_distances = []\n",
    "        for i, cluster in enumerate(clusters):\n",
    "            if len(cluster) == 0:\n",
    "                init_distances.append(0)\n",
    "                continue\n",
    "            \n",
    "            # Find nearest vertex in cluster to agent\n",
    "            agent_pos = agent_positions[i]\n",
    "            min_dist = float('inf')\n",
    "            for v in cluster:\n",
    "                d = abs(v[0] - agent_pos[0]) + abs(v[1] - agent_pos[1])\n",
    "                if d < min_dist:\n",
    "                    min_dist = d\n",
    "            init_distances.append(min_dist)\n",
    "        \n",
    "        # Compute optimal distributed distance\n",
    "        d_star = 2 * total_vertices / self.k\n",
    "        \n",
    "        # Compute target ratios\n",
    "        total_init = sum(init_distances)\n",
    "        ratios = []\n",
    "        for i in range(self.k):\n",
    "            r = (total_init - init_distances[i] + d_star)\n",
    "            ratios.append(r)\n",
    "        \n",
    "        # Normalize\n",
    "        total_r = sum(ratios)\n",
    "        if total_r > 0:\n",
    "            ratios = [r / total_r for r in ratios]\n",
    "        else:\n",
    "            ratios = [1.0 / self.k] * self.k\n",
    "        \n",
    "        return ratios\n",
    "    \n",
    "    def compute_cost(self, clusters: List[Set[Tuple[int, int]]], \n",
    "                     target_ratios: List[float],\n",
    "                     edges: Dict) -> float:\n",
    "        \"\"\"Compute cost function C(P)\"\"\"\n",
    "        total_weight = sum(self.compute_internal_weight(c, edges) for c in clusters)\n",
    "        if total_weight == 0:\n",
    "            total_weight = sum(len(c) for c in clusters)\n",
    "        \n",
    "        cost = 0\n",
    "        for i, cluster in enumerate(clusters):\n",
    "            cluster_weight = self.compute_internal_weight(cluster, edges)\n",
    "            if cluster_weight == 0:\n",
    "                cluster_weight = len(cluster)\n",
    "            \n",
    "            actual_ratio = cluster_weight / max(total_weight, 1)\n",
    "            cost += abs(actual_ratio - target_ratios[i])\n",
    "        \n",
    "        return cost\n",
    "    \n",
    "    def k_step(self, clusters: List[Set[Tuple[int, int]]],\n",
    "               target_ratios: List[float],\n",
    "               edges: Dict) -> List[Set[Tuple[int, int]]]:\n",
    "        \"\"\"K-step: Update cluster assignments for boundary vertices\"\"\"\n",
    "        improved = True\n",
    "        \n",
    "        while improved:\n",
    "            improved = False\n",
    "            \n",
    "            for i, cluster in enumerate(clusters):\n",
    "                boundary_vertices = []\n",
    "                for v in cluster:\n",
    "                    for neighbor in edges.get(v, []):\n",
    "                        if neighbor not in cluster:\n",
    "                            boundary_vertices.append(v)\n",
    "                            break\n",
    "                \n",
    "                for v in boundary_vertices:\n",
    "                    # Find neighboring clusters\n",
    "                    neighbor_clusters = set()\n",
    "                    for neighbor in edges.get(v, []):\n",
    "                        for j, other_cluster in enumerate(clusters):\n",
    "                            if j != i and neighbor in other_cluster:\n",
    "                                neighbor_clusters.add(j)\n",
    "                    \n",
    "                    # Try moving to each neighboring cluster\n",
    "                    current_cost = self.compute_cost(clusters, target_ratios, edges)\n",
    "                    \n",
    "                    for j in neighbor_clusters:\n",
    "                        # Try move\n",
    "                        new_clusters = [set(c) for c in clusters]\n",
    "                        new_clusters[i].remove(v)\n",
    "                        new_clusters[j].add(v)\n",
    "                        \n",
    "                        # Check connectivity\n",
    "                        if len(new_clusters[i]) > 0:\n",
    "                            new_cost = self.compute_cost(new_clusters, target_ratios, edges)\n",
    "                            \n",
    "                            if new_cost < current_cost - 1e-6:\n",
    "                                clusters = new_clusters\n",
    "                                current_cost = new_cost\n",
    "                                improved = True\n",
    "                                break\n",
    "                    \n",
    "                    if improved:\n",
    "                        break\n",
    "                \n",
    "                if improved:\n",
    "                    break\n",
    "        \n",
    "        return clusters\n",
    "    \n",
    "    def merge_isolated_components(self, clusters: List[Set[Tuple[int, int]]],\n",
    "                                   edges: Dict) -> List[Set[Tuple[int, int]]]:\n",
    "        \"\"\"M-step: Merge isolated components\"\"\"\n",
    "        for i, cluster in enumerate(clusters):\n",
    "            if len(cluster) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Find connected components\n",
    "            components = []\n",
    "            remaining = set(cluster)\n",
    "            \n",
    "            while remaining:\n",
    "                start = remaining.pop()\n",
    "                component = {start}\n",
    "                queue = deque([start])\n",
    "                \n",
    "                while queue:\n",
    "                    current = queue.popleft()\n",
    "                    for neighbor in edges.get(current, []):\n",
    "                        if neighbor in remaining:\n",
    "                            remaining.remove(neighbor)\n",
    "                            component.add(neighbor)\n",
    "                            queue.append(neighbor)\n",
    "                \n",
    "                components.append(component)\n",
    "            \n",
    "            # Keep largest component, merge others to neighbors\n",
    "            if len(components) > 1:\n",
    "                main_component = max(components, key=len)\n",
    "                clusters[i] = main_component\n",
    "                \n",
    "                for comp in components:\n",
    "                    if comp != main_component:\n",
    "                        # Find neighboring cluster\n",
    "                        for v in comp:\n",
    "                            for neighbor in edges.get(v, []):\n",
    "                                for j, other_cluster in enumerate(clusters):\n",
    "                                    if j != i and neighbor in other_cluster:\n",
    "                                        clusters[j].update(comp)\n",
    "                                        break\n",
    "        \n",
    "        return clusters\n",
    "    \n",
    "    def generate_spanning_tree_path(self, cluster: Set[Tuple[int, int]],\n",
    "                                     edges: Dict,\n",
    "                                     start: Tuple[int, int]) -> List[Tuple[int, int]]:\n",
    "        \"\"\"Generate coverage path using spanning tree\"\"\"\n",
    "        if len(cluster) == 0:\n",
    "            return []\n",
    "        \n",
    "        if start not in cluster:\n",
    "            start = next(iter(cluster))\n",
    "        \n",
    "        # Build MST using Prim's algorithm\n",
    "        visited = {start}\n",
    "        mst_edges = []\n",
    "        \n",
    "        while len(visited) < len(cluster):\n",
    "            min_edge = None\n",
    "            min_weight = float('inf')\n",
    "            \n",
    "            for v in visited:\n",
    "                for neighbor in edges.get(v, []):\n",
    "                    if neighbor in cluster and neighbor not in visited:\n",
    "                        weight = 1  # Unit weight for grid\n",
    "                        if weight < min_weight:\n",
    "                            min_weight = weight\n",
    "                            min_edge = (v, neighbor)\n",
    "            \n",
    "            if min_edge is None:\n",
    "                break\n",
    "            \n",
    "            mst_edges.append(min_edge)\n",
    "            visited.add(min_edge[1])\n",
    "        \n",
    "        # DFS traversal of MST\n",
    "        mst_adj = defaultdict(list)\n",
    "        for u, v in mst_edges:\n",
    "            mst_adj[u].append(v)\n",
    "            mst_adj[v].append(u)\n",
    "        \n",
    "        path = []\n",
    "        visited = set()\n",
    "        \n",
    "        def dfs(v):\n",
    "            visited.add(v)\n",
    "            path.append(v)\n",
    "            for neighbor in mst_adj[v]:\n",
    "                if neighbor not in visited:\n",
    "                    dfs(neighbor)\n",
    "                    path.append(v)  # Return to v\n",
    "        \n",
    "        dfs(start)\n",
    "        return path\n",
    "    \n",
    "    def solve(self, grid_map: GridMap,\n",
    "              agent_positions: List[Tuple[int, int]] = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Main MCPP solving method\n",
    "        \n",
    "        Returns dict with paths, metrics, and cluster info\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Convert to graph\n",
    "        vertices, edges = self.grid_to_graph(grid_map)\n",
    "        \n",
    "        if len(vertices) == 0:\n",
    "            return {\n",
    "                'paths': [],\n",
    "                'clusters': [],\n",
    "                'execution_time': time.time() - start_time,\n",
    "                'max_path_length': 0,\n",
    "                'total_coverage': 0\n",
    "            }\n",
    "        \n",
    "        # Generate agent positions if not provided\n",
    "        if agent_positions is None:\n",
    "            agent_positions = []\n",
    "            step = len(vertices) // (self.k + 1)\n",
    "            for i in range(self.k):\n",
    "                idx = (i + 1) * step\n",
    "                if idx < len(vertices):\n",
    "                    agent_positions.append(vertices[idx])\n",
    "                else:\n",
    "                    agent_positions.append(vertices[i % len(vertices)])\n",
    "        \n",
    "        # Adjust k if more agents than vertices\n",
    "        self.k = min(self.k, len(vertices))\n",
    "        agent_positions = agent_positions[:self.k]\n",
    "        \n",
    "        # Initialize clusters\n",
    "        clusters = self.initialize_clusters(vertices, edges, agent_positions)\n",
    "        \n",
    "        # Compute target ratios\n",
    "        target_ratios = self.compute_target_ratios(clusters, agent_positions, edges)\n",
    "        \n",
    "        # Iterative optimization\n",
    "        prev_cost = float('inf')\n",
    "        for iteration in range(self.max_iter):\n",
    "            # K-step\n",
    "            clusters = self.k_step(clusters, target_ratios, edges)\n",
    "            \n",
    "            # M-step\n",
    "            clusters = self.merge_isolated_components(clusters, edges)\n",
    "            \n",
    "            # Check convergence\n",
    "            current_cost = self.compute_cost(clusters, target_ratios, edges)\n",
    "            if abs(prev_cost - current_cost) < self.threshold:\n",
    "                break\n",
    "            prev_cost = current_cost\n",
    "        \n",
    "        # Generate paths\n",
    "        paths = []\n",
    "        for i, cluster in enumerate(clusters):\n",
    "            path = self.generate_spanning_tree_path(\n",
    "                cluster, edges, agent_positions[i] if i < len(agent_positions) else None\n",
    "            )\n",
    "            paths.append(path)\n",
    "        \n",
    "        execution_time = time.time() - start_time\n",
    "        \n",
    "        # Compute metrics\n",
    "        path_lengths = [len(p) for p in paths]\n",
    "        max_path_length = max(path_lengths) if path_lengths else 0\n",
    "        total_coverage = sum(len(c) for c in clusters)\n",
    "        \n",
    "        return {\n",
    "            'paths': paths,\n",
    "            'clusters': clusters,\n",
    "            'execution_time': execution_time,\n",
    "            'max_path_length': max_path_length,\n",
    "            'path_lengths': path_lengths,\n",
    "            'total_coverage': total_coverage,\n",
    "            'num_iterations': iteration + 1,\n",
    "            'final_cost': current_cost\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"MCPP Graph-Adapted K-Means implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d657def",
   "metadata": {},
   "source": [
    "## 6. Experimental Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956373b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentRunner:\n",
    "    \"\"\"Run comprehensive experiments comparing algorithms\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.ails = AILSPathfinder()\n",
    "        self.standard = StandardPathfinder()\n",
    "        self.results = []\n",
    "        \n",
    "    def run_single_agent_experiment(self, grid_map: GridMap) -> Dict:\n",
    "        \"\"\"Run single-agent pathfinding comparison\"\"\"\n",
    "        results = {\n",
    "            'grid_size': grid_map.width,\n",
    "            'obstacle_density': grid_map.obstacle_density,\n",
    "            'pattern': grid_map.pattern.value\n",
    "        }\n",
    "        \n",
    "        algorithms = ['astar', 'dijkstra', 'bfs']\n",
    "        \n",
    "        for algo in algorithms:\n",
    "            # Standard algorithm\n",
    "            if algo == 'astar':\n",
    "                std_result = self.standard.astar(grid_map)\n",
    "            elif algo == 'dijkstra':\n",
    "                std_result = self.standard.dijkstra(grid_map)\n",
    "            elif algo == 'bfs':\n",
    "                std_result = self.standard.bfs(grid_map)\n",
    "            \n",
    "            results[f'std_{algo}_time'] = std_result['execution_time'] * 1000  # ms\n",
    "            results[f'std_{algo}_visited'] = std_result['visited_nodes']\n",
    "            results[f'std_{algo}_length'] = std_result['path_length']\n",
    "            \n",
    "            # AILS algorithm\n",
    "            ails_result = self.ails.find_path(grid_map, algo)\n",
    "            results[f'ails_{algo}_time'] = ails_result['execution_time'] * 1000  # ms\n",
    "            results[f'ails_{algo}_visited'] = ails_result['visited_nodes']\n",
    "            results[f'ails_{algo}_length'] = ails_result['path_length']\n",
    "            results[f'ails_{algo}_corridor'] = ails_result['corridor_size']\n",
    "            \n",
    "            # Compute improvements\n",
    "            if std_result['execution_time'] > 0:\n",
    "                time_improvement = ((std_result['execution_time'] - ails_result['execution_time']) / \n",
    "                                    std_result['execution_time']) * 100\n",
    "            else:\n",
    "                time_improvement = 0\n",
    "            results[f'{algo}_time_improvement'] = time_improvement\n",
    "            \n",
    "            if std_result['visited_nodes'] > 0:\n",
    "                visited_improvement = ((std_result['visited_nodes'] - ails_result['visited_nodes']) / \n",
    "                                       std_result['visited_nodes']) * 100\n",
    "            else:\n",
    "                visited_improvement = 0\n",
    "            results[f'{algo}_visited_improvement'] = visited_improvement\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def run_mcpp_experiment(self, grid_map: GridMap, \n",
    "                            num_agents_list: List[int] = [2, 5, 10]) -> List[Dict]:\n",
    "        \"\"\"Run MCPP experiments with varying number of agents\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for k in num_agents_list:\n",
    "            mcpp = MCPPGraphKMeans(num_agents=k)\n",
    "            mcpp_result = mcpp.solve(grid_map)\n",
    "            \n",
    "            result = {\n",
    "                'grid_size': grid_map.width,\n",
    "                'obstacle_density': grid_map.obstacle_density,\n",
    "                'pattern': grid_map.pattern.value,\n",
    "                'num_agents': k,\n",
    "                'mcpp_execution_time': mcpp_result['execution_time'] * 1000,\n",
    "                'mcpp_max_path_length': mcpp_result['max_path_length'],\n",
    "                'mcpp_total_coverage': mcpp_result['total_coverage'],\n",
    "                'mcpp_iterations': mcpp_result['num_iterations']\n",
    "            }\n",
    "            \n",
    "            if mcpp_result['path_lengths']:\n",
    "                result['mcpp_avg_path_length'] = np.mean(mcpp_result['path_lengths'])\n",
    "                result['mcpp_std_path_length'] = np.std(mcpp_result['path_lengths'])\n",
    "            else:\n",
    "                result['mcpp_avg_path_length'] = 0\n",
    "                result['mcpp_std_path_length'] = 0\n",
    "            \n",
    "            results.append(result)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def run_comprehensive_experiments(self, \n",
    "                                       grid_sizes: List[int] = [50, 100, 150, 200],\n",
    "                                       densities: List[float] = [0.1, 0.2, 0.3],\n",
    "                                       patterns: List[ObstaclePattern] = None,\n",
    "                                       num_trials: int = 10) -> pd.DataFrame:\n",
    "        \"\"\"Run comprehensive experiments\"\"\"\n",
    "        if patterns is None:\n",
    "            patterns = [ObstaclePattern.RANDOM, ObstaclePattern.CLUSTERED, \n",
    "                       ObstaclePattern.MAZE, ObstaclePattern.ROOMS, ObstaclePattern.MIXED]\n",
    "        \n",
    "        all_results = []\n",
    "        total_experiments = len(grid_sizes) * len(densities) * len(patterns) * num_trials\n",
    "        experiment_count = 0\n",
    "        \n",
    "        print(f\"Running {total_experiments} experiments...\")\n",
    "        \n",
    "        for size in grid_sizes:\n",
    "            for density in densities:\n",
    "                for pattern in patterns:\n",
    "                    for trial in range(num_trials):\n",
    "                        experiment_count += 1\n",
    "                        if experiment_count % 50 == 0:\n",
    "                            print(f\"Progress: {experiment_count}/{total_experiments}\")\n",
    "                        \n",
    "                        # Generate grid\n",
    "                        grid_map = generate_grid_map(size, size, density, pattern)\n",
    "                        \n",
    "                        # Run single-agent experiments\n",
    "                        result = self.run_single_agent_experiment(grid_map)\n",
    "                        result['trial'] = trial\n",
    "                        all_results.append(result)\n",
    "        \n",
    "        print(\"Experiments complete!\")\n",
    "        return pd.DataFrame(all_results)\n",
    "    \n",
    "    def run_mcpp_experiments(self,\n",
    "                              grid_sizes: List[int] = [50, 100],\n",
    "                              densities: List[float] = [0.1, 0.2],\n",
    "                              patterns: List[ObstaclePattern] = None,\n",
    "                              agent_counts: List[int] = [2, 5, 10, 20],\n",
    "                              num_trials: int = 5) -> pd.DataFrame:\n",
    "        \"\"\"Run MCPP-specific experiments\"\"\"\n",
    "        if patterns is None:\n",
    "            patterns = [ObstaclePattern.RANDOM, ObstaclePattern.CLUSTERED]\n",
    "        \n",
    "        all_results = []\n",
    "        \n",
    "        for size in grid_sizes:\n",
    "            for density in densities:\n",
    "                for pattern in patterns:\n",
    "                    for trial in range(num_trials):\n",
    "                        grid_map = generate_grid_map(size, size, density, pattern)\n",
    "                        mcpp_results = self.run_mcpp_experiment(grid_map, agent_counts)\n",
    "                        \n",
    "                        for result in mcpp_results:\n",
    "                            result['trial'] = trial\n",
    "                            all_results.append(result)\n",
    "        \n",
    "        return pd.DataFrame(all_results)\n",
    "\n",
    "\n",
    "print(\"Experiment runner created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65e3cbf",
   "metadata": {},
   "source": [
    "## 7. Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed82d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize experiment runner\n",
    "runner = ExperimentRunner()\n",
    "\n",
    "# Run comprehensive single-agent experiments (AILS focus)\n",
    "print(\"=\" * 60)\n",
    "print(\"Running Single-Agent Pathfinding Experiments (AILS)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Reduced set for faster execution - adjust as needed\n",
    "single_agent_results = runner.run_comprehensive_experiments(\n",
    "    grid_sizes=[50, 100, 150],\n",
    "    densities=[0.1, 0.2, 0.3],\n",
    "    patterns=[ObstaclePattern.RANDOM, ObstaclePattern.CLUSTERED, \n",
    "              ObstaclePattern.MAZE, ObstaclePattern.ROOMS],\n",
    "    num_trials=5\n",
    ")\n",
    "\n",
    "print(f\"Single-agent results: {len(single_agent_results)} experiments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2e126b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Run MCPP experiments\n",
    "print(\"=\" * 60)\n",
    "print(\"Running MCPP Experiments (Graph-Adapted K-Means)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "mcpp_results = runner.run_mcpp_experiments(\n",
    "    grid_sizes=[50, 100],\n",
    "    densities=[0.1, 0.2, 0.3],\n",
    "    patterns=[ObstaclePattern.RANDOM, ObstaclePattern.CLUSTERED],\n",
    "    agent_counts=[2, 5, 10, 20, 30],\n",
    "    num_trials=5\n",
    ")\n",
    "\n",
    "print(f\"MCPP results: {len(mcpp_results)} experiments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10809d99",
   "metadata": {},
   "source": [
    "## 8. Results Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79524a7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_performance_summary_table(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Create summary table matching Table 2 in AILS paper\"\"\"\n",
    "    \n",
    "    algorithms = ['astar', 'dijkstra', 'bfs']\n",
    "    summary_data = []\n",
    "    \n",
    "    for algo in algorithms:\n",
    "        std_time = df[f'std_{algo}_time'].mean()\n",
    "        ails_time = df[f'ails_{algo}_time'].mean()\n",
    "        std_visited = df[f'std_{algo}_visited'].mean()\n",
    "        ails_visited = df[f'ails_{algo}_visited'].mean()\n",
    "        improvement = df[f'{algo}_time_improvement'].mean()\n",
    "        \n",
    "        summary_data.append({\n",
    "            'Algorithm': algo.upper(),\n",
    "            'Standard Time (ms)': f\"{std_time:.2f}\",\n",
    "            'AILS Time (ms)': f\"{ails_time:.2f}\",\n",
    "            'Standard Visited': f\"{std_visited:.0f}\",\n",
    "            'AILS Visited': f\"{ails_visited:.0f}\",\n",
    "            'Time Improvement (%)': f\"{improvement:.1f}\"\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(summary_data)\n",
    "\n",
    "\n",
    "def create_mcpp_summary_table(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Create summary table for MCPP results matching Lee & Lee paper\"\"\"\n",
    "    \n",
    "    summary = df.groupby(['grid_size', 'num_agents']).agg({\n",
    "        'mcpp_max_path_length': ['mean', 'std'],\n",
    "        'mcpp_execution_time': 'mean',\n",
    "        'mcpp_total_coverage': 'mean'\n",
    "    }).round(2)\n",
    "    \n",
    "    summary.columns = ['Max Path Length', 'Std Dev', 'Execution Time (ms)', 'Coverage']\n",
    "    return summary.reset_index()\n",
    "\n",
    "\n",
    "# Generate summary tables\n",
    "print(\"=\" * 60)\n",
    "print(\"AILS Performance Summary\")\n",
    "print(\"=\" * 60)\n",
    "ails_summary = create_performance_summary_table(single_agent_results)\n",
    "print(ails_summary.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MCPP Performance Summary\")\n",
    "print(\"=\" * 60)\n",
    "mcpp_summary = create_mcpp_summary_table(mcpp_results)\n",
    "print(mcpp_summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48fedcb",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def plot_time_improvement_by_algorithm(df: pd.DataFrame, save_path: str = None):\n",
    "    \"\"\"Plot time improvement for each algorithm\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    algorithms = ['astar', 'dijkstra', 'bfs']\n",
    "    improvements = [df[f'{algo}_time_improvement'].mean() for algo in algorithms]\n",
    "    errors = [df[f'{algo}_time_improvement'].std() for algo in algorithms]\n",
    "    \n",
    "    x = np.arange(len(algorithms))\n",
    "    bars = ax.bar(x, improvements, yerr=errors, capsize=5, \n",
    "                  color=['#3498db', '#e74c3c', '#2ecc71'], alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Algorithm')\n",
    "    ax.set_ylabel('Time Improvement (%)')\n",
    "    ax.set_title('AILS Time Improvement by Algorithm')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([a.upper() for a in algorithms])\n",
    "    ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, imp in zip(bars, improvements):\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{imp:.1f}%',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3), textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_performance_by_grid_size(df: pd.DataFrame, save_path: str = None):\n",
    "    \"\"\"Plot performance metrics by grid size\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    grid_sizes = sorted(df['grid_size'].unique())\n",
    "    \n",
    "    # Time comparison\n",
    "    ax = axes[0, 0]\n",
    "    for algo in ['astar', 'dijkstra', 'bfs']:\n",
    "        std_times = [df[df['grid_size'] == s][f'std_{algo}_time'].mean() for s in grid_sizes]\n",
    "        ails_times = [df[df['grid_size'] == s][f'ails_{algo}_time'].mean() for s in grid_sizes]\n",
    "        ax.plot(grid_sizes, std_times, 'o--', label=f'Std {algo.upper()}', alpha=0.7)\n",
    "        ax.plot(grid_sizes, ails_times, 's-', label=f'AILS {algo.upper()}', alpha=0.7)\n",
    "    ax.set_xlabel('Grid Size')\n",
    "    ax.set_ylabel('Execution Time (ms)')\n",
    "    ax.set_title('Execution Time vs Grid Size')\n",
    "    ax.legend(loc='upper left', fontsize=8)\n",
    "    ax.set_yscale('log')\n",
    "    \n",
    "    # Visited nodes\n",
    "    ax = axes[0, 1]\n",
    "    for algo in ['astar', 'dijkstra', 'bfs']:\n",
    "        std_visited = [df[df['grid_size'] == s][f'std_{algo}_visited'].mean() for s in grid_sizes]\n",
    "        ails_visited = [df[df['grid_size'] == s][f'ails_{algo}_visited'].mean() for s in grid_sizes]\n",
    "        ax.plot(grid_sizes, std_visited, 'o--', label=f'Std {algo.upper()}', alpha=0.7)\n",
    "        ax.plot(grid_sizes, ails_visited, 's-', label=f'AILS {algo.upper()}', alpha=0.7)\n",
    "    ax.set_xlabel('Grid Size')\n",
    "    ax.set_ylabel('Visited Nodes')\n",
    "    ax.set_title('Visited Nodes vs Grid Size')\n",
    "    ax.legend(loc='upper left', fontsize=8)\n",
    "    \n",
    "    # Time improvement by grid size\n",
    "    ax = axes[1, 0]\n",
    "    for algo in ['astar', 'dijkstra', 'bfs']:\n",
    "        improvements = [df[df['grid_size'] == s][f'{algo}_time_improvement'].mean() for s in grid_sizes]\n",
    "        ax.plot(grid_sizes, improvements, 'o-', label=algo.upper(), linewidth=2, markersize=8)\n",
    "    ax.set_xlabel('Grid Size')\n",
    "    ax.set_ylabel('Time Improvement (%)')\n",
    "    ax.set_title('AILS Time Improvement vs Grid Size')\n",
    "    ax.legend()\n",
    "    ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    \n",
    "    # Improvement by obstacle density\n",
    "    ax = axes[1, 1]\n",
    "    densities = sorted(df['obstacle_density'].unique())\n",
    "    for algo in ['astar', 'dijkstra', 'bfs']:\n",
    "        improvements = [df[df['obstacle_density'] == d][f'{algo}_time_improvement'].mean() for d in densities]\n",
    "        ax.plot([d*100 for d in densities], improvements, 'o-', label=algo.upper(), linewidth=2, markersize=8)\n",
    "    ax.set_xlabel('Obstacle Density (%)')\n",
    "    ax.set_ylabel('Time Improvement (%)')\n",
    "    ax.set_title('AILS Time Improvement vs Obstacle Density')\n",
    "    ax.legend()\n",
    "    ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_pattern_comparison(df: pd.DataFrame, save_path: str = None):\n",
    "    \"\"\"Plot performance by obstacle pattern\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    patterns = df['pattern'].unique()\n",
    "    \n",
    "    # Time improvement by pattern\n",
    "    ax = axes[0]\n",
    "    x = np.arange(len(patterns))\n",
    "    width = 0.25\n",
    "    \n",
    "    for i, algo in enumerate(['astar', 'dijkstra', 'bfs']):\n",
    "        improvements = [df[df['pattern'] == p][f'{algo}_time_improvement'].mean() for p in patterns]\n",
    "        ax.bar(x + i*width, improvements, width, label=algo.upper(), alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Obstacle Pattern')\n",
    "    ax.set_ylabel('Time Improvement (%)')\n",
    "    ax.set_title('AILS Time Improvement by Obstacle Pattern')\n",
    "    ax.set_xticks(x + width)\n",
    "    ax.set_xticklabels([p.capitalize() for p in patterns], rotation=45)\n",
    "    ax.legend()\n",
    "    ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    \n",
    "    # Visited reduction by pattern\n",
    "    ax = axes[1]\n",
    "    for i, algo in enumerate(['astar', 'dijkstra', 'bfs']):\n",
    "        reductions = [df[df['pattern'] == p][f'{algo}_visited_improvement'].mean() for p in patterns]\n",
    "        ax.bar(x + i*width, reductions, width, label=algo.upper(), alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Obstacle Pattern')\n",
    "    ax.set_ylabel('Visited Nodes Reduction (%)')\n",
    "    ax.set_title('AILS Visited Nodes Reduction by Pattern')\n",
    "    ax.set_xticks(x + width)\n",
    "    ax.set_xticklabels([p.capitalize() for p in patterns], rotation=45)\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "\n",
    "# Generate plots\n",
    "print(\"\\nGenerating visualization plots...\")\n",
    "\n",
    "fig1 = plot_time_improvement_by_algorithm(single_agent_results, 'ails_time_improvement.png')\n",
    "fig2 = plot_performance_by_grid_size(single_agent_results, 'ails_performance_grid_size.png')\n",
    "fig3 = plot_pattern_comparison(single_agent_results, 'ails_pattern_comparison.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7330baff",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def plot_mcpp_results(df: pd.DataFrame, save_path: str = None):\n",
    "    \"\"\"Plot MCPP experiment results\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Max path length vs number of agents\n",
    "    ax = axes[0, 0]\n",
    "    for size in df['grid_size'].unique():\n",
    "        subset = df[df['grid_size'] == size].groupby('num_agents')['mcpp_max_path_length'].mean()\n",
    "        ax.plot(subset.index, subset.values, 'o-', label=f'{size}x{size}', linewidth=2)\n",
    "    ax.set_xlabel('Number of Agents (k)')\n",
    "    ax.set_ylabel('Max Path Length')\n",
    "    ax.set_title('MCPP: Max Path Length vs Number of Agents')\n",
    "    ax.legend()\n",
    "    \n",
    "    # Execution time vs number of agents\n",
    "    ax = axes[0, 1]\n",
    "    for size in df['grid_size'].unique():\n",
    "        subset = df[df['grid_size'] == size].groupby('num_agents')['mcpp_execution_time'].mean()\n",
    "        ax.plot(subset.index, subset.values, 'o-', label=f'{size}x{size}', linewidth=2)\n",
    "    ax.set_xlabel('Number of Agents (k)')\n",
    "    ax.set_ylabel('Execution Time (ms)')\n",
    "    ax.set_title('MCPP: Execution Time vs Number of Agents')\n",
    "    ax.legend()\n",
    "    \n",
    "    # Standard deviation of path lengths\n",
    "    ax = axes[1, 0]\n",
    "    agent_counts = sorted(df['num_agents'].unique())\n",
    "    for size in df['grid_size'].unique():\n",
    "        stds = [df[(df['grid_size'] == size) & (df['num_agents'] == k)]['mcpp_std_path_length'].mean() \n",
    "                for k in agent_counts]\n",
    "        ax.plot(agent_counts, stds, 'o-', label=f'{size}x{size}', linewidth=2)\n",
    "    ax.set_xlabel('Number of Agents (k)')\n",
    "    ax.set_ylabel('Std Dev of Path Lengths')\n",
    "    ax.set_title('MCPP: Path Length Variation')\n",
    "    ax.legend()\n",
    "    \n",
    "    # Coverage efficiency\n",
    "    ax = axes[1, 1]\n",
    "    for density in df['obstacle_density'].unique():\n",
    "        subset = df[df['obstacle_density'] == density].groupby('num_agents')['mcpp_total_coverage'].mean()\n",
    "        ax.plot(subset.index, subset.values, 'o-', label=f'Density {density*100:.0f}%', linewidth=2)\n",
    "    ax.set_xlabel('Number of Agents (k)')\n",
    "    ax.set_ylabel('Total Vertices Covered')\n",
    "    ax.set_title('MCPP: Coverage by Obstacle Density')\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "\n",
    "fig4 = plot_mcpp_results(mcpp_results, 'mcpp_results.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5bb209",
   "metadata": {},
   "source": [
    "## 9. Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b983d9ba",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def perform_statistical_analysis(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Perform statistical tests comparing AILS vs Standard algorithms\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for algo in ['astar', 'dijkstra', 'bfs']:\n",
    "        std_times = df[f'std_{algo}_time'].values\n",
    "        ails_times = df[f'ails_{algo}_time'].values\n",
    "        \n",
    "        # Paired t-test\n",
    "        t_stat, p_value = stats.ttest_rel(std_times, ails_times)\n",
    "        \n",
    "        # Cohen's d effect size\n",
    "        diff = std_times - ails_times\n",
    "        cohens_d = np.mean(diff) / np.std(diff) if np.std(diff) > 0 else 0\n",
    "        \n",
    "        # Confidence interval\n",
    "        mean_diff = np.mean(diff)\n",
    "        std_diff = np.std(diff)\n",
    "        n = len(diff)\n",
    "        ci_95 = stats.t.interval(0.95, n-1, loc=mean_diff, scale=std_diff/np.sqrt(n))\n",
    "        \n",
    "        results.append({\n",
    "            'Algorithm': algo.upper(),\n",
    "            't-statistic': f\"{t_stat:.2f}\",\n",
    "            'p-value': f\"{p_value:.2e}\" if p_value < 0.001 else f\"{p_value:.4f}\",\n",
    "            'Cohens d': f\"{cohens_d:.2f}\",\n",
    "            '95% CI Lower': f\"{ci_95[0]:.2f}\",\n",
    "            '95% CI Upper': f\"{ci_95[1]:.2f}\",\n",
    "            'Significant': 'Yes' if p_value < 0.001 else 'No'\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def plot_statistical_analysis(df: pd.DataFrame, save_path: str = None):\n",
    "    \"\"\"Create statistical analysis visualization\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    algorithms = ['astar', 'dijkstra', 'bfs']\n",
    "    \n",
    "    # Effect sizes\n",
    "    ax = axes[0]\n",
    "    effect_sizes = []\n",
    "    for algo in algorithms:\n",
    "        std_times = df[f'std_{algo}_time'].values\n",
    "        ails_times = df[f'ails_{algo}_time'].values\n",
    "        diff = std_times - ails_times\n",
    "        cohens_d = np.mean(diff) / np.std(diff) if np.std(diff) > 0 else 0\n",
    "        effect_sizes.append(abs(cohens_d))\n",
    "    \n",
    "    colors = ['#3498db' if d > 0.8 else '#f39c12' if d > 0.5 else '#e74c3c' for d in effect_sizes]\n",
    "    bars = ax.bar(algorithms, effect_sizes, color=colors, alpha=0.8)\n",
    "    ax.axhline(y=0.8, color='green', linestyle='--', label='Large effect (0.8)')\n",
    "    ax.axhline(y=0.5, color='orange', linestyle='--', label='Medium effect (0.5)')\n",
    "    ax.set_xlabel('Algorithm')\n",
    "    ax.set_ylabel(\"Cohen's d (Effect Size)\")\n",
    "    ax.set_title('Effect Size Analysis')\n",
    "    ax.set_xticklabels([a.upper() for a in algorithms])\n",
    "    ax.legend(loc='lower right')\n",
    "    \n",
    "    # Time distribution comparison\n",
    "    ax = axes[1]\n",
    "    positions = []\n",
    "    data = []\n",
    "    labels = []\n",
    "    for i, algo in enumerate(algorithms):\n",
    "        positions.extend([i*3, i*3+1])\n",
    "        data.append(df[f'std_{algo}_time'].values)\n",
    "        data.append(df[f'ails_{algo}_time'].values)\n",
    "        labels.extend([f'Std\\n{algo.upper()}', f'AILS\\n{algo.upper()}'])\n",
    "    \n",
    "    bp = ax.boxplot(data, positions=range(len(data)), patch_artist=True)\n",
    "    colors_box = ['#e74c3c', '#2ecc71'] * len(algorithms)\n",
    "    for patch, color in zip(bp['boxes'], colors_box):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.7)\n",
    "    ax.set_xticklabels(labels, fontsize=9)\n",
    "    ax.set_ylabel('Execution Time (ms)')\n",
    "    ax.set_title('Time Distribution Comparison')\n",
    "    ax.set_yscale('log')\n",
    "    \n",
    "    # P-value heatmap\n",
    "    ax = axes[2]\n",
    "    p_values = []\n",
    "    for algo in algorithms:\n",
    "        std_times = df[f'std_{algo}_time'].values\n",
    "        ails_times = df[f'ails_{algo}_time'].values\n",
    "        _, p = stats.ttest_rel(std_times, ails_times)\n",
    "        p_values.append(-np.log10(max(p, 1e-300)))\n",
    "    \n",
    "    im = ax.bar(algorithms, p_values, color='#9b59b6', alpha=0.8)\n",
    "    ax.axhline(y=-np.log10(0.001), color='red', linestyle='--', label='p=0.001')\n",
    "    ax.axhline(y=-np.log10(0.05), color='orange', linestyle='--', label='p=0.05')\n",
    "    ax.set_xlabel('Algorithm')\n",
    "    ax.set_ylabel('-log10(p-value)')\n",
    "    ax.set_title('Statistical Significance')\n",
    "    ax.set_xticklabels([a.upper() for a in algorithms])\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Statistical Analysis\")\n",
    "print(\"=\" * 60)\n",
    "stat_results = perform_statistical_analysis(single_agent_results)\n",
    "print(stat_results.to_string(index=False))\n",
    "\n",
    "fig5 = plot_statistical_analysis(single_agent_results, 'statistical_analysis.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12889520",
   "metadata": {},
   "source": [
    "## 10. Comparative Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac70b7cc",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_performance_heatmaps(df: pd.DataFrame, save_path: str = None):\n",
    "    \"\"\"Create heatmaps showing performance across grid sizes and densities\"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "    \n",
    "    algorithms = ['astar', 'dijkstra', 'bfs']\n",
    "    grid_sizes = sorted(df['grid_size'].unique())\n",
    "    densities = sorted(df['obstacle_density'].unique())\n",
    "    \n",
    "    for idx, algo in enumerate(algorithms):\n",
    "        # Time improvement heatmap\n",
    "        ax = axes[0, idx]\n",
    "        heatmap_data = np.zeros((len(densities), len(grid_sizes)))\n",
    "        \n",
    "        for i, density in enumerate(densities):\n",
    "            for j, size in enumerate(grid_sizes):\n",
    "                subset = df[(df['grid_size'] == size) & (df['obstacle_density'] == density)]\n",
    "                heatmap_data[i, j] = subset[f'{algo}_time_improvement'].mean()\n",
    "        \n",
    "        im = ax.imshow(heatmap_data, cmap='RdYlGn', aspect='auto', vmin=-20, vmax=80)\n",
    "        ax.set_xticks(range(len(grid_sizes)))\n",
    "        ax.set_xticklabels(grid_sizes)\n",
    "        ax.set_yticks(range(len(densities)))\n",
    "        ax.set_yticklabels([f'{d*100:.0f}%' for d in densities])\n",
    "        ax.set_xlabel('Grid Size')\n",
    "        ax.set_ylabel('Obstacle Density')\n",
    "        ax.set_title(f'{algo.upper()} Time Improvement (%)')\n",
    "        \n",
    "        # Add text annotations\n",
    "        for i in range(len(densities)):\n",
    "            for j in range(len(grid_sizes)):\n",
    "                text = ax.text(j, i, f'{heatmap_data[i, j]:.1f}',\n",
    "                              ha='center', va='center', color='black', fontsize=10)\n",
    "        \n",
    "        plt.colorbar(im, ax=ax)\n",
    "        \n",
    "        # Visited reduction heatmap\n",
    "        ax = axes[1, idx]\n",
    "        heatmap_data = np.zeros((len(densities), len(grid_sizes)))\n",
    "        \n",
    "        for i, density in enumerate(densities):\n",
    "            for j, size in enumerate(grid_sizes):\n",
    "                subset = df[(df['grid_size'] == size) & (df['obstacle_density'] == density)]\n",
    "                heatmap_data[i, j] = subset[f'{algo}_visited_improvement'].mean()\n",
    "        \n",
    "        im = ax.imshow(heatmap_data, cmap='RdYlGn', aspect='auto', vmin=-20, vmax=80)\n",
    "        ax.set_xticks(range(len(grid_sizes)))\n",
    "        ax.set_xticklabels(grid_sizes)\n",
    "        ax.set_yticks(range(len(densities)))\n",
    "        ax.set_yticklabels([f'{d*100:.0f}%' for d in densities])\n",
    "        ax.set_xlabel('Grid Size')\n",
    "        ax.set_ylabel('Obstacle Density')\n",
    "        ax.set_title(f'{algo.upper()} Visited Reduction (%)')\n",
    "        \n",
    "        for i in range(len(densities)):\n",
    "            for j in range(len(grid_sizes)):\n",
    "                text = ax.text(j, i, f'{heatmap_data[i, j]:.1f}',\n",
    "                              ha='center', va='center', color='black', fontsize=10)\n",
    "        \n",
    "        plt.colorbar(im, ax=ax)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "\n",
    "fig6 = create_performance_heatmaps(single_agent_results, 'performance_heatmaps.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0073e3",
   "metadata": {},
   "source": [
    "## 11. Generate Publication-Ready Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2cf491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latex_table(df: pd.DataFrame, caption: str, label: str) -> str:\n",
    "    \"\"\"Generate LaTeX table code for publication\"\"\"\n",
    "    latex = df.to_latex(index=False, escape=False, column_format='l' + 'c' * (len(df.columns) - 1))\n",
    "    \n",
    "    full_latex = f\"\"\"\n",
    "\\\\begin{{table}}[h]\n",
    "\\\\centering\n",
    "\\\\caption{{{caption}}}\n",
    "\\\\label{{{label}}}\n",
    "{latex}\n",
    "\\\\end{{table}}\n",
    "\"\"\"\n",
    "    return full_latex\n",
    "\n",
    "\n",
    "# Generate LaTeX tables\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LaTeX Tables for Publication\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# AILS Performance Table\n",
    "print(\"\\n% AILS Performance Summary Table\")\n",
    "print(generate_latex_table(\n",
    "    ails_summary,\n",
    "    \"Performance Summary Across All Algorithms\",\n",
    "    \"tab:performance_summary\"\n",
    "))\n",
    "\n",
    "# Statistical Analysis Table\n",
    "print(\"\\n% Statistical Analysis Table\")\n",
    "print(generate_latex_table(\n",
    "    stat_results,\n",
    "    \"Statistical Analysis Results\",\n",
    "    \"tab:statistical_analysis\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942a27b2",
   "metadata": {},
   "source": [
    "## 12. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3357cd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Save all results to CSV files\n",
    "single_agent_results.to_csv('ails_single_agent_results.csv', index=False)\n",
    "mcpp_results.to_csv('mcpp_results.csv', index=False)\n",
    "ails_summary.to_csv('ails_summary.csv', index=False)\n",
    "stat_results.to_csv('statistical_analysis.csv', index=False)\n",
    "\n",
    "print(\"Results saved to CSV files!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cc6048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison table\n",
    "def create_algorithm_comparison_table():\n",
    "    \"\"\"Create comparison table between AILS and MCPP approaches\"\"\"\n",
    "    comparison = pd.DataFrame({\n",
    "        'Aspect': [\n",
    "            'Problem Type',\n",
    "            'Agents',\n",
    "            'Optimality',\n",
    "            'Preprocessing',\n",
    "            'Adaptability',\n",
    "            'Grid Type',\n",
    "            'Main Contribution',\n",
    "            'Complexity'\n",
    "        ],\n",
    "        'AILS (This Paper)': [\n",
    "            'Single-agent pathfinding',\n",
    "            'Single',\n",
    "            'Yes (maintains base algorithm)',\n",
    "            'No',\n",
    "            'Yes (obstacle density)',\n",
    "            'Any grid',\n",
    "            'Adaptive corridor width',\n",
    "            'O(nw) density computation'\n",
    "        ],\n",
    "        'MCPP-GAK (Lee & Lee)': [\n",
    "            'Multi-agent coverage',\n",
    "            'Multiple (k)',\n",
    "            'Near-optimal',\n",
    "            'No',\n",
    "            'Yes (graph structure)',\n",
    "            'Grid and non-grid',\n",
    "            'Graph-adapted K-means',\n",
    "            'O(k|V|iterations)'\n",
    "        ]\n",
    "    })\n",
    "    return comparison\n",
    "\n",
    "\n",
    "comparison_table = create_algorithm_comparison_table()\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Algorithm Comparison\")\n",
    "print(\"=\" * 60)\n",
    "print(comparison_table.to_string(index=False))\n",
    "\n",
    "comparison_table.to_csv('algorithm_comparison.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0202fdcf",
   "metadata": {},
   "source": [
    "## 13. Visualization Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7dbbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create example visualizations\n",
    "print(\"\\nGenerating example grid visualizations...\")\n",
    "\n",
    "# Generate sample grids for each pattern\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "patterns = [ObstaclePattern.RANDOM, ObstaclePattern.CLUSTERED, \n",
    "            ObstaclePattern.MAZE, ObstaclePattern.ROOMS, ObstaclePattern.MIXED]\n",
    "\n",
    "for idx, pattern in enumerate(patterns):\n",
    "    grid_map = generate_grid_map(50, 50, 0.2, pattern)\n",
    "    \n",
    "    # Find path with AILS\n",
    "    ails = AILSPathfinder()\n",
    "    result = ails.find_path(grid_map, 'astar')\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    visualize_grid(grid_map, result['path'], result['corridor'], \n",
    "                   f'{pattern.value.capitalize()} Pattern\\n(AILS A*)', ax)\n",
    "\n",
    "# Remove empty subplot\n",
    "axes[5].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('grid_examples.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAll visualizations generated and saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a34fc68",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides a comprehensive comparison between:\n",
    "\n",
    "1. **AILS (Adaptive Incremental Line Search)** - Your single-agent pathfinding optimization\n",
    "2. **MCPP with Graph-Adapted K-Means** - Multi-agent coverage path planning from Lee & Lee (2025)\n",
    "\n",
    "### Key Outputs:\n",
    "- Performance summary tables (Table 2 format from your paper)\n",
    "- Statistical analysis with p-values and effect sizes\n",
    "- Heatmaps showing performance across grid sizes and obstacle densities\n",
    "- Pattern-specific performance analysis\n",
    "- MCPP comparison results\n",
    "- LaTeX tables ready for publication\n",
    "- CSV files for further analysis\n",
    "\n",
    "### Files Generated:\n",
    "- `ails_single_agent_results.csv` - Raw experimental data\n",
    "- `mcpp_results.csv` - MCPP experiment results\n",
    "- `ails_summary.csv` - Performance summary\n",
    "- `statistical_analysis.csv` - Statistical test results\n",
    "- `algorithm_comparison.csv` - Side-by-side comparison\n",
    "- Various PNG figures for publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2abfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EXPERIMENT COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nGenerated files:\")\n",
    "print(\"  - ails_single_agent_results.csv\")\n",
    "print(\"  - mcpp_results.csv\")\n",
    "print(\"  - ails_summary.csv\")\n",
    "print(\"  - statistical_analysis.csv\")\n",
    "print(\"  - algorithm_comparison.csv\")\n",
    "print(\"  - ails_time_improvement.png\")\n",
    "print(\"  - ails_performance_grid_size.png\")\n",
    "print(\"  - ails_pattern_comparison.png\")\n",
    "print(\"  - mcpp_results.png\")\n",
    "print(\"  - statistical_analysis.png\")\n",
    "print(\"  - performance_heatmaps.png\")\n",
    "print(\"  - grid_examples.png\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
