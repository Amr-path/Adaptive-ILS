{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d7b171",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "AILS vs MCPP Comparison - Using Exact Same Data as Lee & Lee (2025)\n",
    "================================================================================\n",
    "\n",
    "This notebook uses the EXACT same benchmark datasets from:\n",
    "\"Multi-Agent Coverage Path Planning Using Graph-Adapted K-Means in Road Network Digital Twin\"\n",
    "Lee & Lee, Electronics 2025, 14, 3921\n",
    "\n",
    "DATA SOURCES:\n",
    "=============\n",
    "\n",
    "1. MOVING AI LAB MAPF BENCHMARKS (Grid Maps):\n",
    "   Download from: https://www.movingai.com/benchmarks/mapf/index.html\n",
    "   \n",
    "   Maps used:\n",
    "   - den312d.map (65x81, 2445 vertices) - Dragon Age game map\n",
    "   - ht_chantry.map (162x141, 7461 vertices) - Game map\n",
    "   - random-64-64-20.map (64x64, 3270 vertices) - 20% random obstacles\n",
    "\n",
    "2. ROAD NETWORK DIGITAL TWINS (Non-Grid):\n",
    "   These are custom datasets. Options to obtain:\n",
    "   a) Contact authors: myungho.lee@pusan.ac.kr\n",
    "   b) Extract from OpenStreetMap using OSMnx library\n",
    "   \n",
    "   Networks used:\n",
    "   - pnu: Pusan National University campus (92 vertices, 112 edges)\n",
    "   - jangjeon: Jangjeon-dong, Busan, South Korea (3997 vertices, 4251 edges)\n",
    "\n",
    "================================================================================\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07c21eb",
   "metadata": {},
   "source": [
    "# How to Get the Exact Same Data as Lee & Lee (2025)\n",
    "\n",
    "## Step 1: Download Moving AI Lab Benchmark Maps\n",
    "\n",
    "Go to: https://www.movingai.com/benchmarks/mapf/index.html\n",
    "\n",
    "Download these specific maps:\n",
    "1. **den312d.map** - [Download Link](https://www.movingai.com/benchmarks/mapf/den312d.map.zip)\n",
    "2. **ht_chantry.map** - [Download Link](https://www.movingai.com/benchmarks/mapf/ht_chantry.map.zip)\n",
    "3. **random-64-64-20.map** - [Download Link](https://www.movingai.com/benchmarks/mapf/random-64-64-20.map.zip)\n",
    "\n",
    "Or download all maps at once: [mapf-map.zip (73KB)](https://www.movingai.com/benchmarks/mapf/mapf-map.zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d50821",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import heapq\n",
    "from collections import deque, defaultdict\n",
    "import time\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from typing import List, Tuple, Dict, Set\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "\n",
    "# For downloading (if network allows)\n",
    "try:\n",
    "    import requests\n",
    "    import zipfile\n",
    "    import io\n",
    "    REQUESTS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    REQUESTS_AVAILABLE = False\n",
    "    print(\"Note: 'requests' not installed. Manual download required.\")\n",
    "\n",
    "# For road network extraction from OpenStreetMap\n",
    "try:\n",
    "    import osmnx as ox\n",
    "    OSMNX_AVAILABLE = True\n",
    "except ImportError:\n",
    "    OSMNX_AVAILABLE = False\n",
    "    print(\"Note: 'osmnx' not installed. Install with: pip install osmnx\")\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'figure.figsize': (12, 8), 'font.size': 12,\n",
    "    'figure.dpi': 150, 'savefig.dpi': 300\n",
    "})\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ca4d63",
   "metadata": {},
   "source": [
    "## Step 2: Load Moving AI Lab Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48325753",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class MovingAILoader:\n",
    "    \"\"\"\n",
    "    Load benchmark maps from Moving AI Lab\n",
    "    Format documentation: https://www.movingai.com/benchmarks/formats.html\n",
    "    \"\"\"\n",
    "    \n",
    "    # Direct download URLs\n",
    "    MAP_URLS = {\n",
    "        'den312d': 'https://www.movingai.com/benchmarks/mapf/den312d.map.zip',\n",
    "        'ht_chantry': 'https://www.movingai.com/benchmarks/mapf/ht_chantry.map.zip',\n",
    "        'random-64-64-20': 'https://www.movingai.com/benchmarks/mapf/random-64-64-20.map.zip',\n",
    "    }\n",
    "    \n",
    "    # Expected specifications (from Lee & Lee paper, Figure 8)\n",
    "    EXPECTED_SPECS = {\n",
    "        'den312d': {'width': 65, 'height': 81, 'vertices': 2445, 'edges': 4391},\n",
    "        'ht_chantry': {'width': 162, 'height': 141, 'vertices': 7461, 'edges': 13963},\n",
    "        'random-64-64-20': {'width': 64, 'height': 64, 'vertices': 3270, 'edges': 5149},\n",
    "    }\n",
    "    \n",
    "    def __init__(self, data_dir: str = './benchmark_data'):\n",
    "        self.data_dir = data_dir\n",
    "        os.makedirs(data_dir, exist_ok=True)\n",
    "    \n",
    "    def download_map(self, map_name: str) -> bool:\n",
    "        \"\"\"Download map from Moving AI Lab\"\"\"\n",
    "        if not REQUESTS_AVAILABLE:\n",
    "            print(f\"Cannot download {map_name}: requests library not available\")\n",
    "            return False\n",
    "        \n",
    "        url = self.MAP_URLS.get(map_name)\n",
    "        if not url:\n",
    "            print(f\"Unknown map: {map_name}\")\n",
    "            return False\n",
    "        \n",
    "        cache_path = os.path.join(self.data_dir, f\"{map_name}.map\")\n",
    "        if os.path.exists(cache_path):\n",
    "            print(f\"Using cached: {map_name}\")\n",
    "            return True\n",
    "        \n",
    "        print(f\"Downloading {map_name}...\", end=\" \")\n",
    "        try:\n",
    "            response = requests.get(url, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "                content = z.read(f\"{map_name}.map\").decode('utf-8')\n",
    "                with open(cache_path, 'w') as f:\n",
    "                    f.write(content)\n",
    "            print(\"OK\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"FAILED: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def parse_map_file(self, filepath: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Parse .map file format:\n",
    "        \n",
    "        type octile\n",
    "        height Y\n",
    "        width X\n",
    "        map\n",
    "        ...grid data...\n",
    "        \n",
    "        Characters:\n",
    "        '.' or 'G' = passable\n",
    "        '@', 'O', 'T', 'W' = obstacle\n",
    "        \"\"\"\n",
    "        with open(filepath, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        height = width = None\n",
    "        map_start = 0\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            line = line.strip()\n",
    "            if line.startswith('height'):\n",
    "                height = int(line.split()[1])\n",
    "            elif line.startswith('width'):\n",
    "                width = int(line.split()[1])\n",
    "            elif line == 'map':\n",
    "                map_start = i + 1\n",
    "                break\n",
    "        \n",
    "        if height is None or width is None:\n",
    "            raise ValueError(\"Could not parse dimensions\")\n",
    "        \n",
    "        grid = np.ones((height, width), dtype=np.int32)  # Default obstacle\n",
    "        passable = {'.', 'G', 'S'}\n",
    "        \n",
    "        for y, line in enumerate(lines[map_start:map_start + height]):\n",
    "            for x, char in enumerate(line[:width]):\n",
    "                if char in passable:\n",
    "                    grid[y, x] = 0\n",
    "        \n",
    "        return grid\n",
    "    \n",
    "    def load_map(self, map_name: str) -> np.ndarray:\n",
    "        \"\"\"Load a map (download if necessary)\"\"\"\n",
    "        cache_path = os.path.join(self.data_dir, f\"{map_name}.map\")\n",
    "        \n",
    "        if not os.path.exists(cache_path):\n",
    "            if not self.download_map(map_name):\n",
    "                print(f\"\\nMANUAL DOWNLOAD REQUIRED:\")\n",
    "                print(f\"1. Go to: {self.MAP_URLS.get(map_name, 'https://www.movingai.com/benchmarks/mapf/')}\")\n",
    "                print(f\"2. Download and extract {map_name}.map\")\n",
    "                print(f\"3. Place it in: {self.data_dir}/\")\n",
    "                return None\n",
    "        \n",
    "        grid = self.parse_map_file(cache_path)\n",
    "        \n",
    "        # Verify against expected specs\n",
    "        spec = self.EXPECTED_SPECS.get(map_name, {})\n",
    "        passable = np.sum(grid == 0)\n",
    "        \n",
    "        print(f\"Loaded {map_name}: {grid.shape[1]}x{grid.shape[0]}, {passable} vertices\")\n",
    "        if spec:\n",
    "            print(f\"  Expected: {spec['width']}x{spec['height']}, {spec['vertices']} vertices\")\n",
    "            if passable != spec['vertices']:\n",
    "                print(f\"  WARNING: Vertex count mismatch!\")\n",
    "        \n",
    "        return grid\n",
    "    \n",
    "    def load_all_benchmark_maps(self) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Load all benchmark maps\"\"\"\n",
    "        maps = {}\n",
    "        for map_name in self.MAP_URLS.keys():\n",
    "            grid = self.load_map(map_name)\n",
    "            if grid is not None:\n",
    "                maps[map_name] = grid\n",
    "        return maps\n",
    "\n",
    "\n",
    "# Try to load Moving AI maps\n",
    "print(\"=\"*60)\n",
    "print(\"Loading Moving AI Lab Benchmark Maps\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "loader = MovingAILoader()\n",
    "benchmark_grids = loader.load_all_benchmark_maps()\n",
    "\n",
    "if not benchmark_grids:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MANUAL DOWNLOAD INSTRUCTIONS\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\"\"\n",
    "1. Visit: https://www.movingai.com/benchmarks/mapf/index.html\n",
    "\n",
    "2. Download these map files:\n",
    "   - den312d.map.zip\n",
    "   - ht_chantry.map.zip\n",
    "   - random-64-64-20.map.zip\n",
    "\n",
    "3. Extract the .map files to: ./benchmark_data/\n",
    "\n",
    "4. Re-run this cell\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb21762",
   "metadata": {},
   "source": [
    "## Step 3: Load/Create Road Network Digital Twins\n",
    "\n",
    "The road network data (pnu, jangjeon) can be obtained by:\n",
    "1. **Contact the authors** at myungho.lee@pusan.ac.kr\n",
    "2. **Extract from OpenStreetMap** using the OSMnx library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87af1f28",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class RoadNetworkLoader:\n",
    "    \"\"\"\n",
    "    Load road network digital twin data\n",
    "    Used in Lee & Lee (2025) for non-grid experiments\n",
    "    \"\"\"\n",
    "    \n",
    "    # Locations for OSM extraction (approximate)\n",
    "    LOCATIONS = {\n",
    "        'pnu': {\n",
    "            'name': 'Pusan National University',\n",
    "            'center': (35.2314, 129.0830),  # Lat, Lon\n",
    "            'dist': 500,  # meters\n",
    "            'expected_vertices': 92,\n",
    "            'expected_edges': 112\n",
    "        },\n",
    "        'jangjeon': {\n",
    "            'name': 'Jangjeon-dong, Busan',\n",
    "            'center': (35.2297, 129.0878),\n",
    "            'dist': 2000,\n",
    "            'expected_vertices': 3997,\n",
    "            'expected_edges': 4251\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def __init__(self, data_dir: str = './benchmark_data'):\n",
    "        self.data_dir = data_dir\n",
    "        os.makedirs(data_dir, exist_ok=True)\n",
    "    \n",
    "    def extract_from_osm(self, location_name: str) -> Tuple[List, Dict, Dict]:\n",
    "        \"\"\"\n",
    "        Extract road network from OpenStreetMap using OSMnx\n",
    "        \n",
    "        Returns:\n",
    "            vertices: List of (x, y) coordinates\n",
    "            edges: Dict mapping vertex to list of neighbors\n",
    "            weights: Dict mapping (u, v) to edge weight (distance)\n",
    "        \"\"\"\n",
    "        if not OSMNX_AVAILABLE:\n",
    "            print(\"OSMnx not available. Install with: pip install osmnx\")\n",
    "            return None, None, None\n",
    "        \n",
    "        loc = self.LOCATIONS.get(location_name)\n",
    "        if not loc:\n",
    "            print(f\"Unknown location: {location_name}\")\n",
    "            return None, None, None\n",
    "        \n",
    "        print(f\"Extracting {loc['name']} from OpenStreetMap...\")\n",
    "        \n",
    "        try:\n",
    "            # Download street network\n",
    "            G = ox.graph_from_point(\n",
    "                loc['center'], \n",
    "                dist=loc['dist'],\n",
    "                network_type='drive'\n",
    "            )\n",
    "            \n",
    "            # Convert to simple graph structure\n",
    "            vertices = []\n",
    "            vertex_map = {}  # OSM node ID -> index\n",
    "            \n",
    "            for i, (node, data) in enumerate(G.nodes(data=True)):\n",
    "                vertices.append((data['x'], data['y']))\n",
    "                vertex_map[node] = i\n",
    "            \n",
    "            edges = defaultdict(list)\n",
    "            weights = {}\n",
    "            \n",
    "            for u, v, data in G.edges(data=True):\n",
    "                if u in vertex_map and v in vertex_map:\n",
    "                    ui, vi = vertex_map[u], vertex_map[v]\n",
    "                    edges[ui].append(vi)\n",
    "                    edges[vi].append(ui)  # Undirected\n",
    "                    weight = data.get('length', 1)\n",
    "                    weights[(ui, vi)] = weight\n",
    "                    weights[(vi, ui)] = weight\n",
    "            \n",
    "            print(f\"  Extracted: {len(vertices)} vertices, {len(weights)//2} edges\")\n",
    "            print(f\"  Expected:  {loc['expected_vertices']} vertices, {loc['expected_edges']} edges\")\n",
    "            \n",
    "            return vertices, dict(edges), weights\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error: {e}\")\n",
    "            return None, None, None\n",
    "    \n",
    "    def create_synthetic_network(self, location_name: str) -> Tuple[List, Dict, Dict]:\n",
    "        \"\"\"\n",
    "        Create synthetic road network matching paper specifications\n",
    "        (Use this if OSM extraction is not possible)\n",
    "        \"\"\"\n",
    "        loc = self.LOCATIONS.get(location_name)\n",
    "        if not loc:\n",
    "            return None, None, None\n",
    "        \n",
    "        n_vertices = loc['expected_vertices']\n",
    "        n_edges = loc['expected_edges']\n",
    "        \n",
    "        print(f\"Creating synthetic {location_name} network...\")\n",
    "        \n",
    "        # Generate random graph with similar structure\n",
    "        np.random.seed(hash(location_name) % 2**32)\n",
    "        \n",
    "        # Generate vertices\n",
    "        vertices = [(np.random.uniform(0, 100), np.random.uniform(0, 100)) \n",
    "                   for _ in range(n_vertices)]\n",
    "        \n",
    "        # Generate edges based on proximity\n",
    "        edges = defaultdict(list)\n",
    "        weights = {}\n",
    "        edge_count = 0\n",
    "        \n",
    "        # Sort vertices for efficient neighbor finding\n",
    "        from scipy.spatial import KDTree\n",
    "        tree = KDTree(vertices)\n",
    "        \n",
    "        for i, v in enumerate(vertices):\n",
    "            # Find k nearest neighbors\n",
    "            distances, indices = tree.query(v, k=min(6, n_vertices))\n",
    "            \n",
    "            for d, j in zip(distances[1:], indices[1:]):  # Skip self\n",
    "                if edge_count >= n_edges:\n",
    "                    break\n",
    "                if j not in edges[i]:\n",
    "                    edges[i].append(j)\n",
    "                    edges[j].append(i)\n",
    "                    weights[(i, j)] = d\n",
    "                    weights[(j, i)] = d\n",
    "                    edge_count += 1\n",
    "            \n",
    "            if edge_count >= n_edges:\n",
    "                break\n",
    "        \n",
    "        print(f\"  Created: {len(vertices)} vertices, {edge_count} edges\")\n",
    "        \n",
    "        return vertices, dict(edges), weights\n",
    "    \n",
    "    def load_network(self, location_name: str, use_osm: bool = True) -> Tuple[List, Dict, Dict]:\n",
    "        \"\"\"Load road network (try OSM first, fall back to synthetic)\"\"\"\n",
    "        \n",
    "        if use_osm and OSMNX_AVAILABLE:\n",
    "            result = self.extract_from_osm(location_name)\n",
    "            if result[0] is not None:\n",
    "                return result\n",
    "        \n",
    "        return self.create_synthetic_network(location_name)\n",
    "\n",
    "\n",
    "# Load road network data\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Loading Road Network Digital Twins\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "road_loader = RoadNetworkLoader()\n",
    "road_networks = {}\n",
    "\n",
    "for loc_name in ['pnu', 'jangjeon']:\n",
    "    print(f\"\\n{loc_name}:\")\n",
    "    vertices, edges, weights = road_loader.load_network(loc_name, use_osm=OSMNX_AVAILABLE)\n",
    "    if vertices:\n",
    "        road_networks[loc_name] = {\n",
    "            'vertices': vertices,\n",
    "            'edges': edges,\n",
    "            'weights': weights\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928dca11",
   "metadata": {},
   "source": [
    "## Step 4: Data Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b5b299",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GridMap:\n",
    "    \"\"\"Grid-based map (for Moving AI benchmarks)\"\"\"\n",
    "    grid: np.ndarray\n",
    "    start: Tuple[int, int]\n",
    "    goal: Tuple[int, int]\n",
    "    width: int\n",
    "    height: int\n",
    "    name: str = \"\"\n",
    "    \n",
    "    def is_valid(self, pos):\n",
    "        x, y = pos\n",
    "        return 0 <= x < self.width and 0 <= y < self.height and self.grid[y, x] == 0\n",
    "    \n",
    "    def get_neighbors(self, pos, diagonal=False):\n",
    "        x, y = pos\n",
    "        dirs = [(-1,0), (1,0), (0,-1), (0,1)]\n",
    "        if diagonal:\n",
    "            dirs += [(-1,-1), (-1,1), (1,-1), (1,1)]\n",
    "        return [(x+dx, y+dy) for dx, dy in dirs if self.is_valid((x+dx, y+dy))]\n",
    "\n",
    "\n",
    "@dataclass  \n",
    "class GraphMap:\n",
    "    \"\"\"Graph-based map (for road networks)\"\"\"\n",
    "    vertices: List[Tuple[float, float]]\n",
    "    edges: Dict[int, List[int]]\n",
    "    weights: Dict[Tuple[int, int], float]\n",
    "    name: str = \"\"\n",
    "    \n",
    "    def get_neighbors(self, node: int) -> List[int]:\n",
    "        return self.edges.get(node, [])\n",
    "    \n",
    "    def get_edge_weight(self, u: int, v: int) -> float:\n",
    "        return self.weights.get((u, v), 1.0)\n",
    "\n",
    "\n",
    "def generate_scenarios(grid: np.ndarray, n: int = 100) -> List[Dict]:\n",
    "    \"\"\"Generate random start/goal scenarios for grid maps\"\"\"\n",
    "    np.random.seed(42)\n",
    "    free = [(x, y) for y in range(grid.shape[0]) for x in range(grid.shape[1]) if grid[y, x] == 0]\n",
    "    \n",
    "    scenarios = []\n",
    "    for i in range(min(n, len(free)//2)):\n",
    "        idx = np.random.choice(len(free), 2, replace=False)\n",
    "        scenarios.append({'id': i, 'start': free[idx[0]], 'goal': free[idx[1]]})\n",
    "    return scenarios\n",
    "\n",
    "\n",
    "# Generate scenarios for each benchmark map\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Generating Test Scenarios\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "benchmark_scenarios = {}\n",
    "for name, grid in benchmark_grids.items():\n",
    "    scenarios = generate_scenarios(grid, n=100)\n",
    "    benchmark_scenarios[name] = scenarios\n",
    "    print(f\"{name}: {len(scenarios)} scenarios\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff80cd2",
   "metadata": {},
   "source": [
    "## Step 5: AILS Algorithm Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34199ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AILS:\n",
    "    \"\"\"Adaptive Incremental Line Search\"\"\"\n",
    "    \n",
    "    def __init__(self, r_min=1, r_max=5, window=7, alpha=1.0):\n",
    "        self.r_min, self.r_max = r_min, r_max\n",
    "        self.window, self.alpha = window, alpha\n",
    "    \n",
    "    def bresenham(self, p0, p1):\n",
    "        x0, y0 = p0\n",
    "        x1, y1 = p1\n",
    "        pts = []\n",
    "        dx, dy = abs(x1-x0), abs(y1-y0)\n",
    "        sx, sy = (1 if x0<x1 else -1), (1 if y0<y1 else -1)\n",
    "        err = dx - dy\n",
    "        while True:\n",
    "            pts.append((x0, y0))\n",
    "            if x0==x1 and y0==y1: break\n",
    "            e2 = 2*err\n",
    "            if e2 > -dy: err -= dy; x0 += sx\n",
    "            if e2 < dx: err += dx; y0 += sy\n",
    "        return pts\n",
    "    \n",
    "    def density(self, gm, pt):\n",
    "        x, y = pt\n",
    "        hw = self.window // 2\n",
    "        obs = tot = 0\n",
    "        for dy in range(-hw, hw+1):\n",
    "            for dx in range(-hw, hw+1):\n",
    "                nx, ny = x+dx, y+dy\n",
    "                if 0 <= nx < gm.width and 0 <= ny < gm.height:\n",
    "                    tot += 1\n",
    "                    if gm.grid[ny, nx] == 1: obs += 1\n",
    "        return obs / max(tot, 1)\n",
    "    \n",
    "    def corridor(self, gm, line):\n",
    "        C = set()\n",
    "        for pt in line:\n",
    "            r = self.r_min + int((self.r_max - self.r_min) * (self.density(gm, pt) ** self.alpha))\n",
    "            r = min(r, self.r_max)\n",
    "            x, y = pt\n",
    "            for dy in range(-r, r+1):\n",
    "                for dx in range(-r, r+1):\n",
    "                    if dx*dx + dy*dy <= r*r:\n",
    "                        nx, ny = x+dx, y+dy\n",
    "                        if 0 <= nx < gm.width and 0 <= ny < gm.height:\n",
    "                            C.add((nx, ny))\n",
    "        return C\n",
    "    \n",
    "    def expand(self, gm, C, n=2):\n",
    "        for _ in range(n):\n",
    "            new = set()\n",
    "            for x, y in C:\n",
    "                for dx, dy in [(-1,0),(1,0),(0,-1),(0,1)]:\n",
    "                    nx, ny = x+dx, y+dy\n",
    "                    if 0 <= nx < gm.width and 0 <= ny < gm.height:\n",
    "                        new.add((nx, ny))\n",
    "            C = C | new\n",
    "        return C\n",
    "    \n",
    "    def astar(self, gm, C):\n",
    "        s, g = gm.start, gm.goal\n",
    "        h = lambda a: abs(a[0]-g[0]) + abs(a[1]-g[1])\n",
    "        \n",
    "        pq = [(0, s)]\n",
    "        came = {}\n",
    "        gscore = {s: 0}\n",
    "        visited = set()\n",
    "        \n",
    "        while pq:\n",
    "            _, cur = heapq.heappop(pq)\n",
    "            if cur in visited: continue\n",
    "            visited.add(cur)\n",
    "            \n",
    "            if cur == g:\n",
    "                path = [cur]\n",
    "                while cur in came:\n",
    "                    cur = came[cur]\n",
    "                    path.append(cur)\n",
    "                return path[::-1], len(visited)\n",
    "            \n",
    "            for nb in gm.get_neighbors(cur, diagonal=True):\n",
    "                if nb not in C or nb in visited: continue\n",
    "                tg = gscore[cur] + 1\n",
    "                if nb not in gscore or tg < gscore[nb]:\n",
    "                    came[nb] = cur\n",
    "                    gscore[nb] = tg\n",
    "                    heapq.heappush(pq, (tg + h(nb), nb))\n",
    "        \n",
    "        return [], len(visited)\n",
    "    \n",
    "    def dijkstra(self, gm, C):\n",
    "        s, g = gm.start, gm.goal\n",
    "        dist = {s: 0}\n",
    "        prev = {}\n",
    "        pq = [(0, s)]\n",
    "        visited = set()\n",
    "        \n",
    "        while pq:\n",
    "            d, cur = heapq.heappop(pq)\n",
    "            if cur in visited: continue\n",
    "            visited.add(cur)\n",
    "            \n",
    "            if cur == g:\n",
    "                path = [cur]\n",
    "                while cur in prev:\n",
    "                    cur = prev[cur]\n",
    "                    path.append(cur)\n",
    "                return path[::-1], len(visited)\n",
    "            \n",
    "            for nb in gm.get_neighbors(cur, diagonal=True):\n",
    "                if nb not in C or nb in visited: continue\n",
    "                nd = dist[cur] + 1\n",
    "                if nb not in dist or nd < dist[nb]:\n",
    "                    dist[nb] = nd\n",
    "                    prev[nb] = cur\n",
    "                    heapq.heappush(pq, (nd, nb))\n",
    "        \n",
    "        return [], len(visited)\n",
    "    \n",
    "    def bfs(self, gm, C):\n",
    "        s, g = gm.start, gm.goal\n",
    "        q = deque([s])\n",
    "        visited = {s}\n",
    "        prev = {}\n",
    "        \n",
    "        while q:\n",
    "            cur = q.popleft()\n",
    "            if cur == g:\n",
    "                path = [cur]\n",
    "                while cur in prev:\n",
    "                    cur = prev[cur]\n",
    "                    path.append(cur)\n",
    "                return path[::-1], len(visited)\n",
    "            \n",
    "            for nb in gm.get_neighbors(cur, diagonal=False):\n",
    "                if nb in C and nb not in visited:\n",
    "                    visited.add(nb)\n",
    "                    prev[nb] = cur\n",
    "                    q.append(nb)\n",
    "        \n",
    "        return [], len(visited)\n",
    "    \n",
    "    def find_path(self, gm, algo='astar'):\n",
    "        t0 = time.time()\n",
    "        line = self.bresenham(gm.start, gm.goal)\n",
    "        C = self.corridor(gm, line)\n",
    "        C.add(gm.start)\n",
    "        C.add(gm.goal)\n",
    "        \n",
    "        methods = {'astar': self.astar, 'dijkstra': self.dijkstra, 'bfs': self.bfs}\n",
    "        path, visited = methods[algo](gm, C)\n",
    "        \n",
    "        exp = 0\n",
    "        while not path and exp < 5:\n",
    "            C = self.expand(gm, C)\n",
    "            path, visited = methods[algo](gm, C)\n",
    "            exp += 1\n",
    "        \n",
    "        return {\n",
    "            'path': path, 'length': len(path) if path else -1,\n",
    "            'visited': visited, 'time': time.time() - t0,\n",
    "            'corridor': len(C), 'expansions': exp\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143a5cfa",
   "metadata": {},
   "source": [
    "## Step 6: Standard Pathfinding (Baselines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5fd01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Standard:\n",
    "    \"\"\"Standard pathfinding algorithms\"\"\"\n",
    "    \n",
    "    def astar(self, gm):\n",
    "        t0 = time.time()\n",
    "        s, g = gm.start, gm.goal\n",
    "        h = lambda a: abs(a[0]-g[0]) + abs(a[1]-g[1])\n",
    "        \n",
    "        pq = [(0, s)]\n",
    "        came = {}\n",
    "        gscore = {s: 0}\n",
    "        visited = set()\n",
    "        \n",
    "        while pq:\n",
    "            _, cur = heapq.heappop(pq)\n",
    "            if cur in visited: continue\n",
    "            visited.add(cur)\n",
    "            \n",
    "            if cur == g:\n",
    "                path = [cur]\n",
    "                while cur in came:\n",
    "                    cur = came[cur]\n",
    "                    path.append(cur)\n",
    "                return {'path': path[::-1], 'length': len(path), \n",
    "                        'visited': len(visited), 'time': time.time() - t0}\n",
    "            \n",
    "            for nb in gm.get_neighbors(cur, diagonal=True):\n",
    "                if nb in visited: continue\n",
    "                tg = gscore[cur] + 1\n",
    "                if nb not in gscore or tg < gscore[nb]:\n",
    "                    came[nb] = cur\n",
    "                    gscore[nb] = tg\n",
    "                    heapq.heappush(pq, (tg + h(nb), nb))\n",
    "        \n",
    "        return {'path': [], 'length': -1, 'visited': len(visited), 'time': time.time() - t0}\n",
    "    \n",
    "    def dijkstra(self, gm):\n",
    "        t0 = time.time()\n",
    "        s, g = gm.start, gm.goal\n",
    "        dist = {s: 0}\n",
    "        prev = {}\n",
    "        pq = [(0, s)]\n",
    "        visited = set()\n",
    "        \n",
    "        while pq:\n",
    "            d, cur = heapq.heappop(pq)\n",
    "            if cur in visited: continue\n",
    "            visited.add(cur)\n",
    "            \n",
    "            if cur == g:\n",
    "                path = [cur]\n",
    "                while cur in prev:\n",
    "                    cur = prev[cur]\n",
    "                    path.append(cur)\n",
    "                return {'path': path[::-1], 'length': len(path),\n",
    "                        'visited': len(visited), 'time': time.time() - t0}\n",
    "            \n",
    "            for nb in gm.get_neighbors(cur, diagonal=True):\n",
    "                if nb in visited: continue\n",
    "                nd = dist[cur] + 1\n",
    "                if nb not in dist or nd < dist[nb]:\n",
    "                    dist[nb] = nd\n",
    "                    prev[nb] = cur\n",
    "                    heapq.heappush(pq, (nd, nb))\n",
    "        \n",
    "        return {'path': [], 'length': -1, 'visited': len(visited), 'time': time.time() - t0}\n",
    "    \n",
    "    def bfs(self, gm):\n",
    "        t0 = time.time()\n",
    "        s, g = gm.start, gm.goal\n",
    "        q = deque([s])\n",
    "        visited = {s}\n",
    "        prev = {}\n",
    "        \n",
    "        while q:\n",
    "            cur = q.popleft()\n",
    "            if cur == g:\n",
    "                path = [cur]\n",
    "                while cur in prev:\n",
    "                    cur = prev[cur]\n",
    "                    path.append(cur)\n",
    "                return {'path': path[::-1], 'length': len(path),\n",
    "                        'visited': len(visited), 'time': time.time() - t0}\n",
    "            \n",
    "            for nb in gm.get_neighbors(cur, diagonal=False):\n",
    "                if nb not in visited:\n",
    "                    visited.add(nb)\n",
    "                    prev[nb] = cur\n",
    "                    q.append(nb)\n",
    "        \n",
    "        return {'path': [], 'length': -1, 'visited': len(visited), 'time': time.time() - t0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f36c93e",
   "metadata": {},
   "source": [
    "## Step 7: MCPP with Graph-Adapted K-Means (Lee & Lee 2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ee725d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCPP:\n",
    "    \"\"\"Multi-Agent Coverage Path Planning with Graph-Adapted K-Means\"\"\"\n",
    "    \n",
    "    def __init__(self, k=2, max_iter=2000, threshold=0.005):\n",
    "        self.k = k\n",
    "        self.max_iter = max_iter\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def grid_to_graph(self, gm):\n",
    "        \"\"\"Convert grid map to graph\"\"\"\n",
    "        V = [(x, y) for y in range(gm.height) for x in range(gm.width) if gm.grid[y, x] == 0]\n",
    "        Vset = set(V)\n",
    "        E = defaultdict(list)\n",
    "        for v in V:\n",
    "            x, y = v\n",
    "            for dx, dy in [(-1,0), (1,0), (0,-1), (0,1)]:\n",
    "                nb = (x+dx, y+dy)\n",
    "                if nb in Vset:\n",
    "                    E[v].append(nb)\n",
    "        return V, dict(E)\n",
    "    \n",
    "    def init_clusters(self, V, E, positions):\n",
    "        \"\"\"Initialize clusters via region growing\"\"\"\n",
    "        clusters = [set() for _ in range(self.k)]\n",
    "        assigned = set()\n",
    "        target = len(V) // self.k\n",
    "        Vset = set(V)\n",
    "        \n",
    "        for i, pos in enumerate(positions):\n",
    "            if pos not in Vset:\n",
    "                pos = min(V, key=lambda v: abs(v[0]-pos[0]) + abs(v[1]-pos[1]))\n",
    "            \n",
    "            q = deque([pos])\n",
    "            while q and len(clusters[i]) < target * 0.8:\n",
    "                cur = q.popleft()\n",
    "                if cur in assigned: continue\n",
    "                clusters[i].add(cur)\n",
    "                assigned.add(cur)\n",
    "                for nb in E.get(cur, []):\n",
    "                    if nb not in assigned:\n",
    "                        q.append(nb)\n",
    "        \n",
    "        # Assign remaining\n",
    "        for v in set(V) - assigned:\n",
    "            best = min(range(self.k), \n",
    "                      key=lambda i: min((abs(v[0]-c[0])+abs(v[1]-c[1]) for c in clusters[i]), \n",
    "                                       default=float('inf')))\n",
    "            clusters[best].add(v)\n",
    "        \n",
    "        return clusters\n",
    "    \n",
    "    def internal_weight(self, cluster, E):\n",
    "        return sum(1 for v in cluster for nb in E.get(v, []) if nb in cluster) // 2\n",
    "    \n",
    "    def cost(self, clusters, ratios, E):\n",
    "        total = sum(self.internal_weight(c, E) or len(c) for c in clusters)\n",
    "        return sum(abs((self.internal_weight(c, E) or len(c)) / max(total, 1) - r) \n",
    "                  for c, r in zip(clusters, ratios))\n",
    "    \n",
    "    def k_step(self, clusters, ratios, E):\n",
    "        \"\"\"K-step: optimize cluster boundaries\"\"\"\n",
    "        improved = True\n",
    "        iters = 0\n",
    "        \n",
    "        while improved and iters < 100:\n",
    "            improved = False\n",
    "            iters += 1\n",
    "            \n",
    "            for i, cluster in enumerate(clusters):\n",
    "                boundary = [v for v in cluster if any(nb not in cluster for nb in E.get(v, []))]\n",
    "                \n",
    "                for v in boundary[:10]:\n",
    "                    neighbor_clusters = set()\n",
    "                    for nb in E.get(v, []):\n",
    "                        for j, other in enumerate(clusters):\n",
    "                            if j != i and nb in other:\n",
    "                                neighbor_clusters.add(j)\n",
    "                    \n",
    "                    curr_cost = self.cost(clusters, ratios, E)\n",
    "                    \n",
    "                    for j in neighbor_clusters:\n",
    "                        new_clusters = [set(c) for c in clusters]\n",
    "                        new_clusters[i].remove(v)\n",
    "                        new_clusters[j].add(v)\n",
    "                        \n",
    "                        if len(new_clusters[i]) > 0:\n",
    "                            new_cost = self.cost(new_clusters, ratios, E)\n",
    "                            if new_cost < curr_cost - 1e-6:\n",
    "                                clusters = new_clusters\n",
    "                                improved = True\n",
    "                                break\n",
    "                    \n",
    "                    if improved: break\n",
    "                if improved: break\n",
    "        \n",
    "        return clusters\n",
    "    \n",
    "    def stc_path(self, cluster, E, start):\n",
    "        \"\"\"Generate spanning tree coverage path\"\"\"\n",
    "        if not cluster: return []\n",
    "        if start not in cluster:\n",
    "            start = next(iter(cluster))\n",
    "        \n",
    "        # Build MST\n",
    "        visited = {start}\n",
    "        mst = []\n",
    "        while len(visited) < len(cluster):\n",
    "            found = False\n",
    "            for v in visited:\n",
    "                for nb in E.get(v, []):\n",
    "                    if nb in cluster and nb not in visited:\n",
    "                        mst.append((v, nb))\n",
    "                        visited.add(nb)\n",
    "                        found = True\n",
    "                        break\n",
    "                if found: break\n",
    "            if not found: break\n",
    "        \n",
    "        # DFS traversal\n",
    "        adj = defaultdict(list)\n",
    "        for u, v in mst:\n",
    "            adj[u].append(v)\n",
    "            adj[v].append(u)\n",
    "        \n",
    "        path = []\n",
    "        seen = set()\n",
    "        \n",
    "        def dfs(v):\n",
    "            seen.add(v)\n",
    "            path.append(v)\n",
    "            for nb in adj[v]:\n",
    "                if nb not in seen:\n",
    "                    dfs(nb)\n",
    "                    path.append(v)\n",
    "        \n",
    "        dfs(start)\n",
    "        return path\n",
    "    \n",
    "    def solve(self, gm, positions=None):\n",
    "        \"\"\"Main MCPP solver\"\"\"\n",
    "        t0 = time.time()\n",
    "        \n",
    "        V, E = self.grid_to_graph(gm)\n",
    "        if not V:\n",
    "            return {'paths': [], 'max_length': 0, 'time': 0}\n",
    "        \n",
    "        self.k = min(self.k, len(V))\n",
    "        \n",
    "        if positions is None:\n",
    "            step = len(V) // (self.k + 1)\n",
    "            positions = [V[(i+1) * step % len(V)] for i in range(self.k)]\n",
    "        positions = positions[:self.k]\n",
    "        \n",
    "        clusters = self.init_clusters(V, E, positions)\n",
    "        \n",
    "        # Compute target ratios\n",
    "        total_v = sum(len(c) for c in clusters)\n",
    "        d_star = 2 * total_v / self.k\n",
    "        init_dists = []\n",
    "        for i, c in enumerate(clusters):\n",
    "            if c:\n",
    "                init_dists.append(min(abs(positions[i][0]-v[0]) + abs(positions[i][1]-v[1]) for v in c))\n",
    "            else:\n",
    "                init_dists.append(0)\n",
    "        \n",
    "        total_init = sum(init_dists)\n",
    "        ratios = [(total_init - init_dists[i] + d_star) for i in range(self.k)]\n",
    "        total_r = sum(ratios)\n",
    "        ratios = [r / total_r for r in ratios] if total_r > 0 else [1/self.k] * self.k\n",
    "        \n",
    "        # Optimize\n",
    "        prev_cost = float('inf')\n",
    "        for iteration in range(self.max_iter):\n",
    "            clusters = self.k_step(clusters, ratios, E)\n",
    "            curr_cost = self.cost(clusters, ratios, E)\n",
    "            if abs(prev_cost - curr_cost) < self.threshold:\n",
    "                break\n",
    "            prev_cost = curr_cost\n",
    "        \n",
    "        # Generate paths\n",
    "        paths = [self.stc_path(c, E, positions[i] if i < len(positions) else None) \n",
    "                for i, c in enumerate(clusters)]\n",
    "        \n",
    "        lengths = [len(p) for p in paths]\n",
    "        \n",
    "        return {\n",
    "            'paths': paths,\n",
    "            'clusters': clusters,\n",
    "            'time': time.time() - t0,\n",
    "            'max_length': max(lengths) if lengths else 0,\n",
    "            'lengths': lengths,\n",
    "            'iterations': iteration + 1\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f03ce2",
   "metadata": {},
   "source": [
    "## Step 8: Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cb9a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(benchmark_grids, benchmark_scenarios, n_scenarios=50):\n",
    "    \"\"\"Run all experiments on benchmark data\"\"\"\n",
    "    \n",
    "    ails = AILS()\n",
    "    std = Standard()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for map_name, grid in benchmark_grids.items():\n",
    "        scenarios = benchmark_scenarios.get(map_name, [])[:n_scenarios]\n",
    "        print(f\"\\n{map_name}: {len(scenarios)} scenarios\")\n",
    "        \n",
    "        for i, scen in enumerate(scenarios):\n",
    "            if i % 10 == 0:\n",
    "                print(f\"  {i}/{len(scenarios)}\", end='\\r')\n",
    "            \n",
    "            gm = GridMap(grid, scen['start'], scen['goal'], grid.shape[1], grid.shape[0], map_name)\n",
    "            \n",
    "            for algo in ['astar', 'dijkstra', 'bfs']:\n",
    "                # Standard\n",
    "                if algo == 'astar':\n",
    "                    std_res = std.astar(gm)\n",
    "                elif algo == 'dijkstra':\n",
    "                    std_res = std.dijkstra(gm)\n",
    "                else:\n",
    "                    std_res = std.bfs(gm)\n",
    "                \n",
    "                # AILS\n",
    "                ails_res = ails.find_path(gm, algo)\n",
    "                \n",
    "                # Compute improvement\n",
    "                time_imp = 0\n",
    "                if std_res['time'] > 0:\n",
    "                    time_imp = (std_res['time'] - ails_res['time']) / std_res['time'] * 100\n",
    "                \n",
    "                visited_imp = 0\n",
    "                if std_res['visited'] > 0:\n",
    "                    visited_imp = (std_res['visited'] - ails_res['visited']) / std_res['visited'] * 100\n",
    "                \n",
    "                results.append({\n",
    "                    'map': map_name,\n",
    "                    'scenario': i,\n",
    "                    'algorithm': algo,\n",
    "                    'std_time': std_res['time'] * 1000,\n",
    "                    'ails_time': ails_res['time'] * 1000,\n",
    "                    'std_visited': std_res['visited'],\n",
    "                    'ails_visited': ails_res['visited'],\n",
    "                    'std_length': std_res['length'],\n",
    "                    'ails_length': ails_res['length'],\n",
    "                    'time_improvement': time_imp,\n",
    "                    'visited_improvement': visited_imp\n",
    "                })\n",
    "        \n",
    "        print(f\"  {len(scenarios)}/{len(scenarios)} done\")\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def run_mcpp_experiments(benchmark_grids, agent_counts=[2, 5, 10, 20, 30, 40], n_trials=10):\n",
    "    \"\"\"Run MCPP experiments matching Lee & Lee paper\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for map_name, grid in benchmark_grids.items():\n",
    "        print(f\"\\n{map_name}:\")\n",
    "        \n",
    "        free = [(x, y) for y in range(grid.shape[0]) for x in range(grid.shape[1]) if grid[y, x] == 0]\n",
    "        \n",
    "        for k in agent_counts:\n",
    "            print(f\"  k={k}: \", end=\"\")\n",
    "            \n",
    "            for trial in range(n_trials):\n",
    "                # Generate agent positions\n",
    "                if trial < n_trials // 2:\n",
    "                    # Arbitrary\n",
    "                    np.random.seed(trial)\n",
    "                    idx = np.random.choice(len(free), min(k, len(free)), replace=False)\n",
    "                    positions = [free[i] for i in idx]\n",
    "                    setting = 'arbitrary'\n",
    "                else:\n",
    "                    # Clutter\n",
    "                    np.random.seed(trial + 1000)\n",
    "                    start_idx = np.random.randint(len(free))\n",
    "                    positions = [free[start_idx]]\n",
    "                    for _ in range(k - 1):\n",
    "                        for fc in free:\n",
    "                            if fc not in positions:\n",
    "                                if abs(fc[0] - positions[0][0]) + abs(fc[1] - positions[0][1]) < 20:\n",
    "                                    positions.append(fc)\n",
    "                                    break\n",
    "                        if len(positions) < k:\n",
    "                            remaining = [f for f in free if f not in positions]\n",
    "                            if remaining:\n",
    "                                positions.append(remaining[0])\n",
    "                    setting = 'clutter'\n",
    "                \n",
    "                if len(positions) < k:\n",
    "                    continue\n",
    "                \n",
    "                gm = GridMap(grid, (0,0), (1,1), grid.shape[1], grid.shape[0], map_name)\n",
    "                \n",
    "                mcpp = MCPP(k, max_iter=2000, threshold=0.005)\n",
    "                res = mcpp.solve(gm, positions)\n",
    "                \n",
    "                results.append({\n",
    "                    'map': map_name,\n",
    "                    'k': k,\n",
    "                    'trial': trial,\n",
    "                    'setting': setting,\n",
    "                    'max_length': res['max_length'],\n",
    "                    'time': res['time'] * 1000,\n",
    "                    'iterations': res['iterations']\n",
    "                })\n",
    "            \n",
    "            print(\"done\")\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Run experiments\n",
    "print(\"=\"*60)\n",
    "print(\"Running Experiments on Benchmark Data\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if benchmark_grids:\n",
    "    # Single-agent experiments\n",
    "    print(\"\\n--- Single-Agent Experiments (AILS) ---\")\n",
    "    ails_results = run_experiments(benchmark_grids, benchmark_scenarios, n_scenarios=50)\n",
    "    \n",
    "    # MCPP experiments\n",
    "    print(\"\\n--- Multi-Agent Experiments (MCPP) ---\")\n",
    "    mcpp_results = run_mcpp_experiments(benchmark_grids, \n",
    "                                        agent_counts=[2, 5, 10, 20, 30, 40],\n",
    "                                        n_trials=10)\n",
    "else:\n",
    "    print(\"No benchmark maps loaded. Please download maps first.\")\n",
    "    ails_results = pd.DataFrame()\n",
    "    mcpp_results = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe7c9fd",
   "metadata": {},
   "source": [
    "## Step 9: Results Tables (Matching Paper Format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e65629",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(ails_results) > 0:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"AILS RESULTS (Table 2 format)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    summary = ails_results.groupby(['map', 'algorithm']).agg({\n",
    "        'std_time': 'mean',\n",
    "        'ails_time': 'mean',\n",
    "        'std_visited': 'mean',\n",
    "        'ails_visited': 'mean',\n",
    "        'time_improvement': 'mean',\n",
    "        'visited_improvement': 'mean'\n",
    "    }).round(2)\n",
    "    \n",
    "    print(summary.to_string())\n",
    "    \n",
    "    # Save to CSV\n",
    "    summary.to_csv('ails_results_summary.csv')\n",
    "    ails_results.to_csv('ails_results_full.csv', index=False)\n",
    "    print(\"\\nSaved: ails_results_summary.csv, ails_results_full.csv\")\n",
    "\n",
    "if len(mcpp_results) > 0:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MCPP RESULTS (Tables 2-3 format from Lee & Lee)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    mcpp_summary = mcpp_results.groupby(['map', 'setting', 'k']).agg({\n",
    "        'max_length': ['mean', 'std'],\n",
    "        'time': 'mean'\n",
    "    }).round(2)\n",
    "    \n",
    "    print(mcpp_summary.to_string())\n",
    "    \n",
    "    # Save to CSV\n",
    "    mcpp_summary.to_csv('mcpp_results_summary.csv')\n",
    "    mcpp_results.to_csv('mcpp_results_full.csv', index=False)\n",
    "    print(\"\\nSaved: mcpp_results_summary.csv, mcpp_results_full.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af890b7",
   "metadata": {},
   "source": [
    "## Step 10: Generate Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760c291f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(ails_results) > 0:\n",
    "    # Plot 1: Time improvement by algorithm and map\n",
    "    fig, axes = plt.subplots(1, len(benchmark_grids), figsize=(5*len(benchmark_grids), 5))\n",
    "    if len(benchmark_grids) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, map_name in enumerate(benchmark_grids.keys()):\n",
    "        ax = axes[idx]\n",
    "        map_df = ails_results[ails_results['map'] == map_name]\n",
    "        \n",
    "        algos = ['astar', 'dijkstra', 'bfs']\n",
    "        imps = [map_df[map_df['algorithm'] == a]['time_improvement'].mean() for a in algos]\n",
    "        errs = [map_df[map_df['algorithm'] == a]['time_improvement'].std() for a in algos]\n",
    "        \n",
    "        bars = ax.bar(range(3), imps, yerr=errs, capsize=5,\n",
    "                     color=['#3498db', '#e74c3c', '#2ecc71'], alpha=0.8)\n",
    "        ax.set_xticks(range(3))\n",
    "        ax.set_xticklabels([a.upper() for a in algos])\n",
    "        ax.set_ylabel('Time Improvement (%)')\n",
    "        ax.set_title(f'{map_name}')\n",
    "        ax.axhline(0, color='black', linewidth=0.5)\n",
    "        \n",
    "        for bar, imp in zip(bars, imps):\n",
    "            ax.annotate(f'{imp:.1f}%', xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n",
    "                       xytext=(0, 3), textcoords=\"offset points\", ha='center', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('ails_time_improvement.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "if len(mcpp_results) > 0:\n",
    "    # Plot 2: MCPP results (matching Figure 9 in Lee & Lee)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Max path length\n",
    "    ax = axes[0]\n",
    "    for map_name in benchmark_grids.keys():\n",
    "        df = mcpp_results[mcpp_results['map'] == map_name]\n",
    "        grouped = df.groupby('k')['max_length'].mean()\n",
    "        ax.plot(grouped.index, grouped.values, 'o-', label=map_name, linewidth=2)\n",
    "    ax.set_xlabel('Number of Agents (k)')\n",
    "    ax.set_ylabel('Max Path Length')\n",
    "    ax.set_title('Max Path Length vs k')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Runtime\n",
    "    ax = axes[1]\n",
    "    for map_name in benchmark_grids.keys():\n",
    "        df = mcpp_results[mcpp_results['map'] == map_name]\n",
    "        grouped = df.groupby('k')['time'].mean()\n",
    "        ax.plot(grouped.index, grouped.values, 'o-', label=map_name, linewidth=2)\n",
    "    ax.set_xlabel('Number of Agents (k)')\n",
    "    ax.set_ylabel('Runtime (ms)')\n",
    "    ax.set_title('Runtime vs k')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('mcpp_results.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb76b7d6",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook replicates the experimental setup from Lee & Lee (2025) using:\n",
    "\n",
    "**Grid Maps (from Moving AI Lab):**\n",
    "- den312d (6581, 2445 vertices)\n",
    "- ht_chantry (162141, 7461 vertices)\n",
    "- random-64-64-20 (6464, 3270 vertices)\n",
    "\n",
    "**Road Networks (optional, requires OSMnx):**\n",
    "- pnu (Pusan National University)\n",
    "- jangjeon (Jangjeon-dong, Busan)\n",
    "\n",
    "**To get exact same data:**\n",
    "1. Download maps from https://www.movingai.com/benchmarks/mapf/\n",
    "2. For road networks, contact authors or extract from OpenStreetMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9456adf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FILES GENERATED\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "Results:\n",
    "  - ails_results_summary.csv\n",
    "  - ails_results_full.csv\n",
    "  - mcpp_results_summary.csv\n",
    "  - mcpp_results_full.csv\n",
    "\n",
    "Figures:\n",
    "  - benchmark_maps.png\n",
    "  - ails_time_improvement.png\n",
    "  - mcpp_results.png\n",
    "\n",
    "To get EXACT same data as Lee & Lee (2025):\n",
    "  1. Download from: https://www.movingai.com/benchmarks/mapf/\n",
    "  2. Place .map files in ./benchmark_data/\n",
    "  3. Re-run this notebook\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
